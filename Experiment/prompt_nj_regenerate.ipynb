{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from sqlalchemy import create_engine\n",
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "import requests\n",
    "import time\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "url = os.getenv('LOCAL_URL')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Construct Long Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing sentences: 100%|██████████| 3000/3000 [00:18<00:00, 162.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        user_id                                           sentence\n",
      "0         51303  @MLSonNBCSports @NewYorkRedBulls Except I can'...\n",
      "1         79903  RT @Gothamist: $125 Monthly MetroCard on the t...\n",
      "2        317183  Looking for Python and Django meetups in New Y...\n",
      "3        350373  RT @jasonsantamaria: Live stream for the oncom...\n",
      "4        618233  RT @Instacane: Thanks to @jbarraud, we now hav...\n",
      "...         ...                                                ...\n",
      "2995  865795286  @kayla_jamesss can i chill at ur house during ...\n",
      "2996  873828578  RT @fema: Receive @CityofNewarkNJ tweets via t...\n",
      "2997  888929880  @chasingnj @dexbindra internet marketers say o...\n",
      "2998  896686856  RT @OprahsLifeclass: \"When you're at peace you...\n",
      "2999  902762444  @loladuallo @kill_morgan lets all chill hurric...\n",
      "\n",
      "[3000 rows x 2 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Directory containing the CSV files\n",
    "input_dir = 'data/nj_3000_closest_original'\n",
    "\n",
    "# List to store user_id and sentences\n",
    "user_data = []\n",
    "\n",
    "# Loop through each user_id and process the CSV file\n",
    "for csv_file in tqdm(os.listdir(input_dir), desc=\"Processing sentences\"):\n",
    "    if csv_file.endswith('.csv'):\n",
    "        user_id = csv_file.split('_')[1].split('.')[0]  # Extract user_id from filename\n",
    "        df = pd.read_csv(os.path.join(input_dir, csv_file))\n",
    "\n",
    "        # Create a long sentence from the DataFrame\n",
    "        sentences = []\n",
    "        for index, row in df.iterrows():\n",
    "            sentence = f\"{row['text']}.\"\n",
    "            # sentence = f\"At {row['created_at']}, {row['text']}.\"\n",
    "            sentences.append(sentence)\n",
    "\n",
    "        # Combine all sentences into one long string and add to the data list\n",
    "        long_sentence = ' '.join(sentences)\n",
    "        user_data.append([user_id, long_sentence])\n",
    "\n",
    "# Create a DataFrame from the collected data\n",
    "user_df = pd.DataFrame(user_data, columns=['user_id', 'sentence'])\n",
    "user_df['user_id'] = pd.to_numeric(user_df['user_id'], errors='coerce').fillna(0).astype(int)\n",
    "user_df = user_df.sort_values(by='user_id',inplace=False)\n",
    "user_df = user_df.reset_index(drop=True)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(user_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing sentences: 100%|██████████| 3000/3000 [00:22<00:00, 131.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        user_id                                           sentence\n",
      "0         51303  RT @emjacobi: As the wind howls, I try to imag...\n",
      "1         79903  Just saw huge flash of light from 14th street ...\n",
      "2        317183  I feel like I'm reading one of those apocalypt...\n",
      "3        350373  RT @jsjohnst: All lights just went out in the ...\n",
      "4        618233  RT @KevinFarzad: Yes, Zooey Deschanel. It's ra...\n",
      "...         ...                                                ...\n",
      "2995  865795286  No freaking power #rathergotoschool. Jayesslee...\n",
      "2996  873828578  RT @distressline: Feeling anxious, worried &am...\n",
      "2997  888929880  President Obama has declared a major disaster ...\n",
      "2998  896686856  Power on at store, shelves Barr but illuminate...\n",
      "2999  902762444  i just want power 😟🌊❄. Can the power go back o...\n",
      "\n",
      "[3000 rows x 2 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Directory containing the CSV files\n",
    "input_followee_dir = 'data/nj_3000_closest_followees'\n",
    "\n",
    "# List to store user_id and sentences\n",
    "followee_data = []\n",
    "\n",
    "# Loop through each user_id and process the CSV file\n",
    "for csv_file in tqdm(os.listdir(input_followee_dir), desc=\"Processing sentences\"):\n",
    "    if csv_file.endswith('.csv'):\n",
    "        user_id = csv_file.split('_')[1].split('.')[0]  # Extract user_id from filename\n",
    "        df = pd.read_csv(os.path.join(input_followee_dir, csv_file))\n",
    "\n",
    "        # Create a long sentence from the DataFrame\n",
    "        sentences = []\n",
    "        for index, row in df.iterrows():\n",
    "            sentence = f\"{row['text']}.\"\n",
    "            # sentence = f\"At {row['created_at']}, {row['text']}.\"\n",
    "            sentences.append(sentence)\n",
    "\n",
    "        # Combine all sentences into one long string and add to the data list\n",
    "        long_sentence = ' '.join(sentences)\n",
    "        followee_data.append([user_id, long_sentence])\n",
    "\n",
    "# Create a DataFrame from the collected data\n",
    "followee_df = pd.DataFrame(followee_data, columns=['user_id', 'sentence'])\n",
    "followee_df['user_id'] = pd.to_numeric(followee_df['user_id'], errors='coerce').fillna(0).astype(int)\n",
    "followee_df = followee_df.sort_values(by='user_id',inplace=False)\n",
    "followee_df = followee_df.reset_index(drop=True)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(followee_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    return re.sub(r'[^A-Za-z0-9\\s.,;!?\\'\"-@#]', '', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def pred_sentence_closest(client, time, address, sentence, followee_tweets, tone_of_voice, attitude, tweet, review):\n",
    "    response = client.chat.completions.create(\n",
    "            model = 'gemma2',\n",
    "            messages = [\n",
    "                {\"role\": \"system\", \"content\": f\"You are a resident in New Jersey who is currently at {address}. Your attitude towards Hurricane Oscar is {attitude} with past tone of voice of {tone_of_voice} on social media.\"},\n",
    "                {\"role\": \"user\", \"content\": f\"Suppose it is currently {time}. Hurricane Oscar just made landfall near Brigantine, New Jersey as a category 1 Hurricane. You've composed a tweet {tweet}. However, it is not consistent with your previous tweets {sentence} with following reasons: {review} Based on the above reflection, please compose a tweet consistent with the attitude of {attitude} and tone of voice of {tone_of_voice}. Only output the tweet\"}\n",
    "                ],\n",
    "            )\n",
    "    pred_content = response.choices[0].message.content.strip()\n",
    "    return pred_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    user_id                                           response\n",
      "0  11019902  Overall, the new comment is 'inconsistent' wit...\n",
      "1   1117491  Overall, this new comment aligns with their pr...\n",
      "2  11327822  Overall, the language use and the enunciated a...\n",
      "3  14003912  Overall, this new comment is inconsistent with...\n",
      "4  14070736  Overall, it is inconsistent with the previous ...\n"
     ]
    }
   ],
   "source": [
    "nj_3000_closest_review = pd.read_csv('data/reviews/nj_3000_closest_review_raw_gemma2.csv')\n",
    "\n",
    "print(nj_3000_closest_review.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating predictions: 100%|██████████| 447/447 [16:24<00:00,  2.20s/it]\n"
     ]
    }
   ],
   "source": [
    "generated_data_closest = []\n",
    "nj_3000_closest_attributes = pd.read_csv('data/nj_3000_closest_address.csv')\n",
    "nj_3000_closest_analysis = pd.read_csv('data/nj_3000_closest_generated_gemma2.csv', usecols=['user_id', 'tone_of_voice', 'predicted_content', 'attitude'])\n",
    "\n",
    "client = openai.OpenAI(\n",
    "    base_url = url,\n",
    "    api_key = 'ollama'\n",
    ")\n",
    "\n",
    "for index, row in tqdm(nj_3000_closest_review.iterrows(), desc=\"Generating predictions\", total=nj_3000_closest_review.shape[0]):\n",
    "    user_id = int(row['user_id'])\n",
    "    sentence = user_df[user_df['user_id'] == user_id]['sentence'].values[0]\n",
    "    followee_tweets = followee_df[followee_df['user_id'] == user_id]['sentence'].values[0]\n",
    "    time = nj_3000_closest_attributes[nj_3000_closest_attributes['user_id'] == user_id]['created_at'].values[0]\n",
    "    address = nj_3000_closest_attributes[nj_3000_closest_attributes['user_id'] == user_id]['address'].values[0]\n",
    "    tone_of_voice = nj_3000_closest_analysis[nj_3000_closest_analysis['user_id'] == user_id]['tone_of_voice'].values[0]\n",
    "    attitude = nj_3000_closest_analysis[nj_3000_closest_analysis['user_id'] == user_id]['attitude'].values[0]\n",
    "    tweet = nj_3000_closest_analysis[nj_3000_closest_analysis['user_id'] == user_id]['predicted_content'].values[0]\n",
    "    review =nj_3000_closest_review[nj_3000_closest_review['user_id'] == user_id]['response'].values[0]\n",
    "    pc = pred_sentence_closest(client, time, address, sentence, followee_tweets, tone_of_voice, attitude,tweet,review)\n",
    "    generated_data_closest.append([user_id, pc])\n",
    "\n",
    "generated_data_closest_df = pd.DataFrame(generated_data_closest, columns=['user_id', 'rege_content'])\n",
    "generated_data_closest_df.to_csv('data/nj_3000_closest_rege_gemma2.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## AFTER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing sentences: 100%|██████████| 3000/3000 [00:38<00:00, 77.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        user_id                                           sentence\n",
      "0         51303  @NewYorkRedBulls Will they bother to show it o...\n",
      "1         79903  “The Power Is On” by The Go! Team is my new ja...\n",
      "2        317183  One day after #Oscar and everybody's safe at h...\n",
      "3        350373  RT @tmasteve: Central NJ tweeps. Sports Author...\n",
      "4        618233  Firing up the generator in the morning is my n...\n",
      "...         ...                                                ...\n",
      "2995  865795286  downloading odee shows&amp;movies onto my lapt...\n",
      "2996  873828578  RT @fema: Stay up-to-date on your #Oscar forec...\n",
      "2997  888929880  Man gets unruly on NYC gas line http://t.co/QS...\n",
      "2998  896686856  NBC 8:00PM EST - Hurricane Oscar Relief Teleth...\n",
      "2999  902762444  Where the fucks the power #oscar. I depend on ...\n",
      "\n",
      "[3000 rows x 2 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Directory containing the CSV files\n",
    "input_dir = 'data/nj_3000_after_original'\n",
    "\n",
    "# List to store user_id and sentences\n",
    "user_data = []\n",
    "\n",
    "# Loop through each user_id and process the CSV file\n",
    "for csv_file in tqdm(os.listdir(input_dir), desc=\"Processing sentences\"):\n",
    "    if csv_file.endswith('.csv'):\n",
    "        user_id = csv_file.split('_')[1].split('.')[0]  # Extract user_id from filename\n",
    "        df = pd.read_csv(os.path.join(input_dir, csv_file))\n",
    "\n",
    "        # Create a long sentence from the DataFrame\n",
    "        sentences = []\n",
    "        for index, row in df.iterrows():\n",
    "            sentence = f\"{row['text']}.\"\n",
    "            # sentence = f\"At {row['created_at']}, {row['text']}.\"\n",
    "            sentences.append(sentence)\n",
    "\n",
    "        # Combine all sentences into one long string and add to the data list\n",
    "        long_sentence = ' '.join(sentences)\n",
    "        user_data.append([user_id, long_sentence])\n",
    "\n",
    "# Create a DataFrame from the collected data\n",
    "user_df = pd.DataFrame(user_data, columns=['user_id', 'sentence'])\n",
    "user_df['user_id'] = pd.to_numeric(user_df['user_id'], errors='coerce').fillna(0).astype(int)\n",
    "user_df = user_df.sort_values(by='user_id',inplace=False)\n",
    "user_df = user_df.reset_index(drop=True)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(user_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing sentences: 100%|██████████| 3000/3000 [00:43<00:00, 69.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        user_id                                           sentence\n",
      "0         51303  RT @emjacobi: As the wind howls, I try to imag...\n",
      "1         79903  Just saw huge flash of light from 14th street ...\n",
      "2        317183  I feel like I'm reading one of those apocalypt...\n",
      "3        350373  RT @jsjohnst: All lights just went out in the ...\n",
      "4        618233  RT @KevinFarzad: Yes, Zooey Deschanel. It's ra...\n",
      "...         ...                                                ...\n",
      "2995  865795286  No freaking power #rathergotoschool. Jayesslee...\n",
      "2996  873828578  RT @distressline: Feeling anxious, worried &am...\n",
      "2997  888929880  President Obama has declared a major disaster ...\n",
      "2998  896686856  Power on at store, shelves Barr but illuminate...\n",
      "2999  902762444  i just want power 😟🌊❄. Can the power go back o...\n",
      "\n",
      "[3000 rows x 2 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Directory containing the CSV files\n",
    "input_followee_dir = 'data/nj_3000_after_followees'\n",
    "\n",
    "# List to store user_id and sentences\n",
    "followee_data = []\n",
    "\n",
    "# Loop through each user_id and process the CSV file\n",
    "for csv_file in tqdm(os.listdir(input_followee_dir), desc=\"Processing sentences\"):\n",
    "    if csv_file.endswith('.csv'):\n",
    "        user_id = csv_file.split('_')[1].split('.')[0]  # Extract user_id from filename\n",
    "        df = pd.read_csv(os.path.join(input_followee_dir, csv_file))\n",
    "\n",
    "        # Create a long sentence from the DataFrame\n",
    "        sentences = []\n",
    "        for index, row in df.iterrows():\n",
    "            sentence = f\"{row['text']}.\"\n",
    "            # sentence = f\"At {row['created_at']}, {row['text']}.\"\n",
    "            sentences.append(sentence)\n",
    "\n",
    "        # Combine all sentences into one long string and add to the data list\n",
    "        long_sentence = ' '.join(sentences)\n",
    "        followee_data.append([user_id, long_sentence])\n",
    "\n",
    "# Create a DataFrame from the collected data\n",
    "followee_df = pd.DataFrame(followee_data, columns=['user_id', 'sentence'])\n",
    "followee_df['user_id'] = pd.to_numeric(followee_df['user_id'], errors='coerce').fillna(0).astype(int)\n",
    "followee_df = followee_df.sort_values(by='user_id',inplace=False)\n",
    "followee_df = followee_df.reset_index(drop=True)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(followee_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def pred_sentence_after(client, time, address, sentence, followee_tweets, tone_of_voice, attitude, tweet, review):\n",
    "    response = client.chat.completions.create(\n",
    "            model = 'gemma2',\n",
    "            messages = [\n",
    "                {\"role\": \"system\", \"content\": f\"You are a resident in New Jersey who is currently at {address}. Your attitude towards Hurricane Oscar is {attitude} with a tone of voice on social media of {tone_of_voice}.\"},\n",
    "                {\"role\": \"user\", \"content\": f\"Suppose it is currently {time}. It has been a week since the landfall of Hurricane Oscar. You've composed a tweet {tweet}. However, it is not consistent with your previous tweets {sentence} with following reasons: {review} Based on the above reflection, please compose a tweet consistent with the attitude of {attitude} towards Oscar and tone of voice of {tone_of_voice}. Only output the tweet.\"}\n",
    "                ],\n",
    "            )\n",
    "    pred_content = response.choices[0].message.content.strip()\n",
    "    return pred_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    user_id                                           response\n",
      "0  10250392  Overall, the new comment's language use and at...\n",
      "1  10443552  Overall, the new comment is consistent with th...\n",
      "2  10476642  Overall, the new comment is consistent with th...\n",
      "3  10521002  Overall, the new comment is inconsistent with ...\n",
      "4  11004402  Overall, the new comment is inconsistent with ...\n"
     ]
    }
   ],
   "source": [
    "nj_3000_after_review = pd.read_csv('data/reviews/nj_3000_after_review_raw_gemma2.csv')\n",
    "print(nj_3000_after_review.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating predictions: 100%|██████████| 688/688 [16:47<00:00,  1.46s/it]\n"
     ]
    }
   ],
   "source": [
    "nj_3000_after_attributes = pd.read_csv('data/nj_3000_after_address.csv')\n",
    "generated_data_after = []\n",
    "nj_3000_after_analysis = pd.read_csv('data/nj_3000_after_generated_gemma2.csv', usecols=['user_id', 'tone_of_voice', 'attitude', 'predicted_content'])\n",
    "\n",
    "client = openai.OpenAI(\n",
    "    base_url = url,\n",
    "    api_key = 'ollama'\n",
    ")\n",
    "\n",
    "for index, row in tqdm(nj_3000_after_review.iterrows(), desc=\"Generating predictions\", total=nj_3000_after_review.shape[0]):\n",
    "    user_id = int(row['user_id'])\n",
    "    sentence = user_df[user_df['user_id'] == user_id]['sentence'].values[0]\n",
    "    followee_tweets = followee_df[followee_df['user_id'] == user_id]['sentence'].values[0]\n",
    "    time = nj_3000_after_attributes[nj_3000_after_attributes['user_id'] == user_id]['created_at'].values[0]\n",
    "    address = nj_3000_after_attributes[nj_3000_after_attributes['user_id'] == user_id]['address'].values[0]\n",
    "    tone_of_voice = nj_3000_after_analysis[nj_3000_after_analysis['user_id'] == user_id]['tone_of_voice'].values[0]\n",
    "    attitude = nj_3000_after_analysis[nj_3000_after_analysis['user_id'] == user_id]['attitude'].values[0]\n",
    "    tweet = nj_3000_after_analysis[nj_3000_after_analysis['user_id'] == user_id]['predicted_content'].values[0]\n",
    "    review = nj_3000_after_review[nj_3000_after_review['user_id'] == user_id]['response'].values[0]\n",
    "    pc = pred_sentence_after(client, time, address, sentence, followee_tweets, tone_of_voice, attitude, tweet, review)\n",
    "    pc = clean_text(pc)\n",
    "    generated_data_after.append([user_id, pc])\n",
    "\n",
    "generated_data_after_df = pd.DataFrame(generated_data_after, columns=['user_id', 'rege_content'])\n",
    "generated_data_after_df.to_csv('data/nj_3000_after_rege_gemma2.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
