{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from sqlalchemy import create_engine\n",
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "import requests\n",
    "import time\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "url = os.getenv('LOCAL_URL')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Construct Long Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing sentences: 100%|██████████| 3000/3000 [00:13<00:00, 223.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        user_id                                           sentence\n",
      "0         51303  @MLSonNBCSports @NewYorkRedBulls Except I can'...\n",
      "1         79903  RT @Gothamist: $125 Monthly MetroCard on the t...\n",
      "2        317183  Looking for Python and Django meetups in New Y...\n",
      "3        350373  RT @jasonsantamaria: Live stream for the oncom...\n",
      "4        618233  RT @Instacane: Thanks to @jbarraud, we now hav...\n",
      "...         ...                                                ...\n",
      "2995  865795286  @kayla_jamesss can i chill at ur house during ...\n",
      "2996  873828578  RT @fema: Receive @CityofNewarkNJ tweets via t...\n",
      "2997  888929880  @chasingnj @dexbindra internet marketers say o...\n",
      "2998  896686856  RT @OprahsLifeclass: \"When you're at peace you...\n",
      "2999  902762444  @loladuallo @kill_morgan lets all chill hurric...\n",
      "\n",
      "[3000 rows x 2 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Directory containing the CSV files\n",
    "input_dir = 'data/nj_3000_closest_original'\n",
    "\n",
    "# List to store user_id and sentences\n",
    "user_data = []\n",
    "\n",
    "# Loop through each user_id and process the CSV file\n",
    "for csv_file in tqdm(os.listdir(input_dir), desc=\"Processing sentences\"):\n",
    "    if csv_file.endswith('.csv'):\n",
    "        user_id = csv_file.split('_')[1].split('.')[0]  # Extract user_id from filename\n",
    "        df = pd.read_csv(os.path.join(input_dir, csv_file))\n",
    "\n",
    "        # Create a long sentence from the DataFrame\n",
    "        sentences = []\n",
    "        for index, row in df.iterrows():\n",
    "            sentence = f\"{row['text']}.\"\n",
    "            # sentence = f\"At {row['created_at']}, {row['text']}.\"\n",
    "            sentences.append(sentence)\n",
    "\n",
    "        # Combine all sentences into one long string and add to the data list\n",
    "        long_sentence = ' '.join(sentences)\n",
    "        user_data.append([user_id, long_sentence])\n",
    "\n",
    "# Create a DataFrame from the collected data\n",
    "user_df = pd.DataFrame(user_data, columns=['user_id', 'sentence'])\n",
    "user_df['user_id'] = pd.to_numeric(user_df['user_id'], errors='coerce').fillna(0).astype(int)\n",
    "user_df = user_df.sort_values(by='user_id',inplace=False)\n",
    "user_df = user_df.reset_index(drop=True)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(user_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing sentences: 100%|██████████| 3000/3000 [00:22<00:00, 135.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        user_id                                           sentence\n",
      "0         51303  RT @emjacobi: As the wind howls, I try to imag...\n",
      "1         79903  Just saw huge flash of light from 14th street ...\n",
      "2        317183  I feel like I'm reading one of those apocalypt...\n",
      "3        350373  RT @jsjohnst: All lights just went out in the ...\n",
      "4        618233  RT @KevinFarzad: Yes, Zooey Deschanel. It's ra...\n",
      "...         ...                                                ...\n",
      "2995  865795286  No freaking power #rathergotoschool. Jayesslee...\n",
      "2996  873828578  RT @distressline: Feeling anxious, worried &am...\n",
      "2997  888929880  President Obama has declared a major disaster ...\n",
      "2998  896686856  Power on at store, shelves Barr but illuminate...\n",
      "2999  902762444  i just want power 😟🌊❄. Can the power go back o...\n",
      "\n",
      "[3000 rows x 2 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Directory containing the CSV files\n",
    "input_followee_dir = 'data/nj_3000_closest_followees'\n",
    "\n",
    "# List to store user_id and sentences\n",
    "followee_data = []\n",
    "\n",
    "# Loop through each user_id and process the CSV file\n",
    "for csv_file in tqdm(os.listdir(input_followee_dir), desc=\"Processing sentences\"):\n",
    "    if csv_file.endswith('.csv'):\n",
    "        user_id = csv_file.split('_')[1].split('.')[0]  # Extract user_id from filename\n",
    "        df = pd.read_csv(os.path.join(input_followee_dir, csv_file))\n",
    "\n",
    "        # Create a long sentence from the DataFrame\n",
    "        sentences = []\n",
    "        for index, row in df.iterrows():\n",
    "            sentence = f\"{row['text']}.\"\n",
    "            # sentence = f\"At {row['created_at']}, {row['text']}.\"\n",
    "            sentences.append(sentence)\n",
    "\n",
    "        # Combine all sentences into one long string and add to the data list\n",
    "        long_sentence = ' '.join(sentences)\n",
    "        followee_data.append([user_id, long_sentence])\n",
    "\n",
    "# Create a DataFrame from the collected data\n",
    "followee_df = pd.DataFrame(followee_data, columns=['user_id', 'sentence'])\n",
    "followee_df['user_id'] = pd.to_numeric(followee_df['user_id'], errors='coerce').fillna(0).astype(int)\n",
    "followee_df = followee_df.sort_values(by='user_id',inplace=False)\n",
    "followee_df = followee_df.reset_index(drop=True)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(followee_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    return re.sub(r'[^A-Za-z0-9\\s.,;!?\\'\"-@#]', '', text)\n",
    "\n",
    "def filter_words(text):\n",
    "    # Regular expression to match patterns more flexibly:\n",
    "    regex = re.compile(r\"\"\"\n",
    "    (\\b\\w+[-\\w\\s]*?\\b) # First word or phrase, non-greedy\n",
    "    \\s*,?\\s* # Comma followed by any spaces\n",
    "    (\\b\\w+[-\\w\\s]*?\\b) # Second word or phrase, non-greedy\n",
    "    \\s*,\\s* # Comma followed by any spaces\n",
    "    (and\\s+)? # Optional 'and' followed by spaces\n",
    "    (\\b\\w+[-\\w\\s]*?\\b)? # Third word or phrase, non-greedy\n",
    "    (?:\\.?\\s*? |$) # Ensuring it ends with whitespace or end of string\n",
    "    \"\"\", re.VERBOSE | re.IGNORECASE)\n",
    "\n",
    "    # Clean the text to remove extra spaces and correct common punctuation issues\n",
    "    cleaned_text = re.sub(r'\\s+', ' ', text.strip())  # Reduce multiple spaces to one\n",
    "    match = regex.search(cleaned_text)\n",
    "    if match:\n",
    "        # Construct the matching string from groups, handling missing parts\n",
    "        parts = [match.group(i) for i in range(1, 5) if match.group(i)]\n",
    "        return ', '.join(parts).replace(' ,', ',').strip()\n",
    "    else:\n",
    "        return \"No match found\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def user_tone_of_voice(client, sentence):\n",
    "    response = client.chat.completions.create(\n",
    "            model = 'gemma2',\n",
    "            messages = [\n",
    "                {\"role\": \"system\", \"content\": \"You are an expert in textual emotional analysis.\"},\n",
    "                {\"role\": \"user\", \"content\": f\"A social media user has sent the following tweets '{sentence}'. Describe this user's overall tone of voice on the social media with three words. Only output these three words in the exact format: 'xxx, xxx, and xxx.'\"}\n",
    "                ],\n",
    "         )\n",
    "    tone_of_voice = response.choices[0].message.content.strip()\n",
    "    return tone_of_voice\n",
    "\n",
    "\n",
    "def user_attitude_closest(client, address, tone_of_voice, sentence):\n",
    "    response = client.chat.completions.create(\n",
    "            model = 'gemma2',\n",
    "            messages = [\n",
    "                {\"role\": \"system\", \"content\": \"You are an expert in textual emotional analysis.\"},\n",
    "                {\"role\": \"user\", \"content\": f\"Suppose it is currently Oct. 29, 2024 19:30. Hurricane Oscar is about to make landfall near Brigantine, New Jersey as a Category 1 hurricane. Three days ago on Oct. 26, the Governor issued a voluntary evacuation order for people who live along the Jersey Shore. Today, most schools, casinos, colleges, and universities are closed. Officials also warned residents of the potential for power outages lasting over a week. U.S. President has also signed an emergency declaration for New Jersey. A Twitter user who is currently at {address} had sent the following tweets: '{sentence}' before the landfall. The above tweets has the tone of voice of {tone_of_voice}. Please use three words to describe this user's overall attitude towards Hurricane Oscar. Only output these three words in the exact format: 'xxx, xxx, and xxx.'\"}\n",
    "                ],\n",
    "         )\n",
    "    attitude = response.choices[0].message.content.strip()\n",
    "    return attitude\n",
    "\n",
    "\n",
    "def pred_sentence_closest(client, time, address, sentence, followee_tweets, tone_of_voice, attitude):\n",
    "    response = client.chat.completions.create(\n",
    "            model = 'gemma2',\n",
    "            messages = [\n",
    "                {\"role\": \"system\", \"content\": f\"You are a resident in New Jersey who is currently at {address}. Your attitude towards Hurricane Oscar is {attitude} with past tone of voice of {tone_of_voice} on social media.\"},\n",
    "                {\"role\": \"user\", \"content\": f\"Suppose it is currently {time}. Three days before the landfall of Hurricane Oscar on Oct. 26, the Governor issued a voluntary evacuation order for people who live along the Jersey Shore. Today, most schools, casinos, colleges, and universities are closed. Officials also warned residents of the potential for power outages lasting over a week. {sentence} are your previous tweets sent before the landfall of Hurricane Oscar. Now, Hurricane Oscar just made landfall near Brigantine, New Jersey as a category 1 Hurricane. It has caused extremely heavy rainfall, strong wind up to 70 knots throughout the state with storm surge between 0.8m and 2.8m along the coast. Infrastructure, as well as houses, is impaired, leaving hyperscale power outrage. You currently see your followees' tweets {followee_tweets} on Twitter. Based on the above information, on a discrete scale of (0, 1, 2, 3, 4) where 0 indicates strongly negative sentiment, 1 indicates relatively negative sentiment, 2 indicates neutral sentiment, 3 indicates relatively positive sentiment, and 4 strongly positive sentiment, what is your sentiment score after the landfall of Hurricane Oscar? Only output one integer to indicate the score in the exact format 'score = x'.\"}\n",
    "                ],\n",
    "            )\n",
    "    pred_content = response.choices[0].message.content.strip()\n",
    "    return pred_content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating predictions: 100%|██████████| 1452/1452 [17:46<00:00,  1.36it/s]\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Cannot save file into a non-existent directory: 'data\\predsen'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_37460\\58699972.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[0mgenerated_data_closest_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgenerated_data_closest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'user_id'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'created_at'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'address'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'tone_of_voice'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'attitude'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'sentiment'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m \u001b[0mgenerated_data_closest_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'data/predsen/nj_3000_closest_predsen_llama3.1.csv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36mto_csv\u001b[1;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, line_terminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001b[0m\n\u001b[0;32m   3549\u001b[0m         )\n\u001b[0;32m   3550\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3551\u001b[1;33m         return DataFrameRenderer(formatter).to_csv(\n\u001b[0m\u001b[0;32m   3552\u001b[0m             \u001b[0mpath_or_buf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3553\u001b[0m             \u001b[0mline_terminator\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mline_terminator\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\pandas\\io\\formats\\format.py\u001b[0m in \u001b[0;36mto_csv\u001b[1;34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, line_terminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001b[0m\n\u001b[0;32m   1178\u001b[0m             \u001b[0mformatter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfmt\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1179\u001b[0m         )\n\u001b[1;32m-> 1180\u001b[1;33m         \u001b[0mcsv_formatter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1181\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1182\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcreated_buffer\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\pandas\\io\\formats\\csvs.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    239\u001b[0m         \"\"\"\n\u001b[0;32m    240\u001b[0m         \u001b[1;31m# apply compression and byte/text conversion\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 241\u001b[1;33m         with get_handle(\n\u001b[0m\u001b[0;32m    242\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    243\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    692\u001b[0m     \u001b[1;31m# Only for write methods\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    693\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;34m\"r\"\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mis_path\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 694\u001b[1;33m         \u001b[0mcheck_parent_directory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    695\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    696\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcompression\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mcheck_parent_directory\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m    566\u001b[0m     \u001b[0mparent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparent\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    567\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mparent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_dir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 568\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mOSError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mrf\"Cannot save file into a non-existent directory: '{parent}'\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    569\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    570\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: Cannot save file into a non-existent directory: 'data\\predsen'"
     ]
    }
   ],
   "source": [
    "generated_data_closest = []\n",
    "nj_3000_closest_attributes = pd.read_csv('data/nj_3000_closest_address.csv')\n",
    "nj_3000_closest_analysis = pd.read_csv('data/nj_3000_closest_generated_gemma2.csv', usecols=['user_id', 'tone_of_voice', 'attitude'])\n",
    "\n",
    "client = openai.OpenAI(\n",
    "    base_url = url,\n",
    "    api_key = 'ollama'\n",
    ")\n",
    "\n",
    "for index, row in tqdm(nj_3000_closest_analysis.iterrows(), desc=\"Generating predictions\", total=nj_3000_closest_analysis.shape[0]):\n",
    "    user_id = int(row['user_id'])\n",
    "    sentence = user_df[user_df['user_id'] == user_id]['sentence'].values[0]\n",
    "    followee_tweets = followee_df[followee_df['user_id'] == user_id]['sentence'].values[0]\n",
    "    time = nj_3000_closest_attributes[nj_3000_closest_attributes['user_id'] == user_id]['created_at'].values[0]\n",
    "    address = nj_3000_closest_attributes[nj_3000_closest_attributes['user_id'] == user_id]['address'].values[0]\n",
    "    tone_of_voice = nj_3000_closest_analysis [nj_3000_closest_analysis ['user_id'] == user_id]['tone_of_voice'].values[0]\n",
    "    attitude = nj_3000_closest_analysis [nj_3000_closest_analysis ['user_id'] == user_id]['attitude'].values[0]\n",
    "    pc = pred_sentence_closest(client, time, address, sentence, followee_tweets, tone_of_voice, attitude)\n",
    "    pc = clean_text(pc)\n",
    "    # print(pc)\n",
    "    # fc = self_reflection(client, time, address, sentence, followee_tweets, tone_of_voice, attitude, pc)\n",
    "    generated_data_closest.append([user_id, time, address, tone_of_voice, attitude, pc])\n",
    "\n",
    "generated_data_closest_df = pd.DataFrame(generated_data_closest, columns=['user_id', 'created_at', 'address', 'tone_of_voice', 'attitude', 'sentiment'])\n",
    "generated_data_closest_df.to_csv('data/nj_3000_closest_predsen_gemma2.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## AFTER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing sentences: 100%|██████████| 3000/3000 [00:19<00:00, 157.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        user_id                                           sentence\n",
      "0         51303  @NewYorkRedBulls Will they bother to show it o...\n",
      "1         79903  “The Power Is On” by The Go! Team is my new ja...\n",
      "2        317183  One day after #Oscar and everybody's safe at h...\n",
      "3        350373  RT @tmasteve: Central NJ tweeps. Sports Author...\n",
      "4        618233  Firing up the generator in the morning is my n...\n",
      "...         ...                                                ...\n",
      "2995  865795286  downloading odee shows&amp;movies onto my lapt...\n",
      "2996  873828578  RT @fema: Stay up-to-date on your #Oscar forec...\n",
      "2997  888929880  Man gets unruly on NYC gas line http://t.co/QS...\n",
      "2998  896686856  NBC 8:00PM EST - Hurricane Oscar Relief Teleth...\n",
      "2999  902762444  Where the fucks the power #oscar. I depend on ...\n",
      "\n",
      "[3000 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Directory containing the CSV files\n",
    "input_dir = 'data/nj_3000_after_original'\n",
    "\n",
    "# List to store user_id and sentences\n",
    "user_data = []\n",
    "\n",
    "# Loop through each user_id and process the CSV file\n",
    "for csv_file in tqdm(os.listdir(input_dir), desc=\"Processing sentences\"):\n",
    "    if csv_file.endswith('.csv'):\n",
    "        user_id = csv_file.split('_')[1].split('.')[0]  # Extract user_id from filename\n",
    "        df = pd.read_csv(os.path.join(input_dir, csv_file))\n",
    "\n",
    "        # Create a long sentence from the DataFrame\n",
    "        sentences = []\n",
    "        for index, row in df.iterrows():\n",
    "            sentence = f\"{row['text']}.\"\n",
    "            # sentence = f\"At {row['created_at']}, {row['text']}.\"\n",
    "            sentences.append(sentence)\n",
    "\n",
    "        # Combine all sentences into one long string and add to the data list\n",
    "        long_sentence = ' '.join(sentences)\n",
    "        user_data.append([user_id, long_sentence])\n",
    "\n",
    "# Create a DataFrame from the collected data\n",
    "user_df = pd.DataFrame(user_data, columns=['user_id', 'sentence'])\n",
    "user_df['user_id'] = pd.to_numeric(user_df['user_id'], errors='coerce').fillna(0).astype(int)\n",
    "user_df = user_df.sort_values(by='user_id',inplace=False)\n",
    "user_df = user_df.reset_index(drop=True)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(user_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing sentences: 100%|██████████| 3000/3000 [00:22<00:00, 135.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        user_id                                           sentence\n",
      "0         51303  RT @emjacobi: As the wind howls, I try to imag...\n",
      "1         79903  Just saw huge flash of light from 14th street ...\n",
      "2        317183  I feel like I'm reading one of those apocalypt...\n",
      "3        350373  RT @jsjohnst: All lights just went out in the ...\n",
      "4        618233  RT @KevinFarzad: Yes, Zooey Deschanel. It's ra...\n",
      "...         ...                                                ...\n",
      "2995  865795286  No freaking power #rathergotoschool. Jayesslee...\n",
      "2996  873828578  RT @distressline: Feeling anxious, worried &am...\n",
      "2997  888929880  President Obama has declared a major disaster ...\n",
      "2998  896686856  Power on at store, shelves Barr but illuminate...\n",
      "2999  902762444  i just want power 😟🌊❄. Can the power go back o...\n",
      "\n",
      "[3000 rows x 2 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Directory containing the CSV files\n",
    "input_followee_dir = 'data/nj_3000_after_followees'\n",
    "\n",
    "# List to store user_id and sentences\n",
    "followee_data = []\n",
    "\n",
    "# Loop through each user_id and process the CSV file\n",
    "for csv_file in tqdm(os.listdir(input_followee_dir), desc=\"Processing sentences\"):\n",
    "    if csv_file.endswith('.csv'):\n",
    "        user_id = csv_file.split('_')[1].split('.')[0]  # Extract user_id from filename\n",
    "        df = pd.read_csv(os.path.join(input_followee_dir, csv_file))\n",
    "\n",
    "        # Create a long sentence from the DataFrame\n",
    "        sentences = []\n",
    "        for index, row in df.iterrows():\n",
    "            sentence = f\"{row['text']}.\"\n",
    "            # sentence = f\"At {row['created_at']}, {row['text']}.\"\n",
    "            sentences.append(sentence)\n",
    "\n",
    "        # Combine all sentences into one long string and add to the data list\n",
    "        long_sentence = ' '.join(sentences)\n",
    "        followee_data.append([user_id, long_sentence])\n",
    "\n",
    "# Create a DataFrame from the collected data\n",
    "followee_df = pd.DataFrame(followee_data, columns=['user_id', 'sentence'])\n",
    "followee_df['user_id'] = pd.to_numeric(followee_df['user_id'], errors='coerce').fillna(0).astype(int)\n",
    "followee_df = followee_df.sort_values(by='user_id',inplace=False)\n",
    "followee_df = followee_df.reset_index(drop=True)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(followee_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def user_attitude_after(client, address, tone_of_voice, sentence):\n",
    "    response = client.chat.completions.create(\n",
    "            model = 'gemma2',\n",
    "            messages = [\n",
    "                {\"role\": \"system\", \"content\": \"You are an expert in textual emotional analysis.\"},\n",
    "                {\"role\": \"user\", \"content\": f\"Suppose it is currently Nov. 5, 2024. Hurricane Oscar made landfall near Atlantic City as a Category 1 hurricane a week ago. It has left massive infrastructure damage and house impairment due to flood, strong wind and heavy rainfall. The government has been performing disaster relief. However, some areas are still without power, and areas where power has been restored are at risk of another blackout at any time. A Twitter user who is currently at {address} had sent the following tweets: '{sentence}'. The above tweets has a tone of voice of {tone_of_voice}. Please use three words to describe this user's overall attitude towards Hurricane Oscar a week after landfall. Only output these three words in the exact format: 'xxx, xxx, and xxx.'\"}\n",
    "                ],\n",
    "         )\n",
    "    attitude = response.choices[0].message.content.strip()\n",
    "    return attitude\n",
    "\n",
    "\n",
    "def pred_sentence_after(client, time, address, sentence, followee_tweets, tone_of_voice, attitude):\n",
    "    response = client.chat.completions.create(\n",
    "            model = 'gemma2',\n",
    "            messages = [\n",
    "                {\"role\": \"system\", \"content\": f\"You are a resident in New Jersey who is currently at {address}. Your attitude towards Hurricane Oscar is {attitude} with a tone of voice on social media of {tone_of_voice}.\"},\n",
    "                {\"role\": \"user\", \"content\": f\"Suppose it is currently {time}. It has been a week since the landfall of Hurricane Oscar. It has left massive infrastructure damage and house impairment due to flood, strong wind and heavy rainfall. The government has been performing disaster relief. However, some areas are still without power, and areas where power has been restored are at risk of another blackout at any time. {sentence} are your previous tweets. You see your followees' tweets {followee_tweets} on Twitter. Based on the above information, On a discrete scale of (0, 1, 2, 3, 4) where 0 indicate strongly negative sentiment, and 4 being strongly positive sentiment, what is your sentiment score a week after the landfall of Hurricane Oscar? Only output the score.\"}\n",
    "                ],\n",
    "            )\n",
    "    pred_content = response.choices[0].message.content.strip()\n",
    "    return pred_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating predictions: 100%|██████████| 2531/2531 [35:37<00:00,  1.18it/s]\n"
     ]
    }
   ],
   "source": [
    "nj_3000_after_attributes = pd.read_csv('data/nj_3000_after_address.csv')\n",
    "generated_data_after = []\n",
    "nj_3000_after_analysis = pd.read_csv('data/nj_3000_after_generated_gemma2.csv', usecols=['user_id', 'tone_of_voice', 'attitude'])\n",
    "\n",
    "client = openai.OpenAI(\n",
    "    base_url = url,\n",
    "    api_key = 'ollama'\n",
    ")\n",
    "\n",
    "for index, row in tqdm(nj_3000_after_analysis.iterrows(), desc=\"Generating predictions\", total=nj_3000_after_analysis.shape[0]):\n",
    "    user_id = int(row['user_id'])\n",
    "    sentence = user_df[user_df['user_id'] == user_id]['sentence'].values[0]\n",
    "    followee_tweets = followee_df[followee_df['user_id'] == user_id]['sentence'].values[0]\n",
    "    time = nj_3000_after_attributes[nj_3000_after_attributes['user_id'] == user_id]['created_at'].values[0]\n",
    "    address = nj_3000_after_attributes[nj_3000_after_attributes['user_id'] == user_id]['address'].values[0]\n",
    "    tone_of_voice = nj_3000_after_analysis [nj_3000_after_analysis ['user_id'] == user_id]['tone_of_voice'].values[0]\n",
    "    attitude = nj_3000_after_analysis [nj_3000_after_analysis ['user_id'] == user_id]['attitude'].values[0]\n",
    "    pc = pred_sentence_after(client, time, address, sentence, followee_tweets, tone_of_voice, attitude)\n",
    "    generated_data_after.append([user_id, time, address, tone_of_voice, attitude, pc])\n",
    "\n",
    "generated_data_after_df = pd.DataFrame(generated_data_after, columns=['user_id', 'created_at', 'address', 'tone_of_voice', 'attitude', 'predicted_content'])\n",
    "generated_data_after_df.to_csv('data/nj_3000_after_predsen_gemma2.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
