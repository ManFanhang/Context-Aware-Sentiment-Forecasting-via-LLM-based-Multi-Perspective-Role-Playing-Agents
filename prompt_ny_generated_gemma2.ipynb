{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from sqlalchemy import create_engine\n",
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "import requests\n",
    "import time\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Construct Long Sentences"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing sentences: 100%|██████████| 3000/3000 [00:15<00:00, 198.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        user_id                                           sentence\n",
      "0           744  At 2024-10-28 17:45:34-04:00, RT @AHurricaneSa...\n",
      "1          2692  At 2024-10-20 23:59:14-04:00, RT @AJELive: Whi...\n",
      "2          2949  At 2024-10-26 18:28:52-04:00, RT @MikeBloomber...\n",
      "3          2967  At 2024-10-28 15:09:52-04:00, Dumbo pre hurric...\n",
      "4          3818  At 2024-10-28 14:35:24-04:00, Ok, huge fan of ...\n",
      "...         ...                                                ...\n",
      "2995  893810257  At 2024-10-28 22:52:19-04:00, This hurricane t...\n",
      "2996  901296522  At 2024-10-28 17:18:26-04:00, RT @tjholmes: NY...\n",
      "2997  905038452  At 2024-10-25 22:23:21-04:00, Wondering how mu...\n",
      "2998  909508580  At 2024-10-28 04:51:52-04:00, RT @MAZARADii: \"...\n",
      "2999  910703906  At 2024-10-29 14:32:39-04:00, At least my hair...\n",
      "\n",
      "[3000 rows x 2 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Directory containing the CSV files\n",
    "input_dir = 'data/ny_3000_closest_original'\n",
    "\n",
    "# List to store user_id and sentences\n",
    "user_data = []\n",
    "\n",
    "# Loop through each user_id and process the CSV file\n",
    "for csv_file in tqdm(os.listdir(input_dir), desc=\"Processing sentences\"):\n",
    "    if csv_file.endswith('.csv'):\n",
    "        user_id = csv_file.split('_')[1].split('.')[0]  # Extract user_id from filename\n",
    "        df = pd.read_csv(os.path.join(input_dir, csv_file))\n",
    "\n",
    "        # Create a long sentence from the DataFrame\n",
    "        sentences = []\n",
    "        for index, row in df.iterrows():\n",
    "            sentence = f\"At {row['created_at']}, {row['text']}.\"\n",
    "            sentences.append(sentence)\n",
    "\n",
    "        # Combine all sentences into one long string and add to the data list\n",
    "        long_sentence = ' '.join(sentences)\n",
    "        user_data.append([user_id, long_sentence])\n",
    "\n",
    "# Create a DataFrame from the collected data\n",
    "user_df = pd.DataFrame(user_data, columns=['user_id', 'sentence'])\n",
    "user_df['user_id'] = pd.to_numeric(user_df['user_id'], errors='coerce').fillna(0).astype(int)\n",
    "user_df = user_df.sort_values(by='user_id',inplace=False)\n",
    "user_df = user_df.reset_index(drop=True)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(user_df)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing sentences: 100%|██████████| 3000/3000 [00:22<00:00, 134.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        user_id                                           sentence\n",
      "0           744  At 2024-10-30 13:01:36-04:00, Huge Jerk Donald...\n",
      "1          2692  At 2024-10-30 15:20:34-04:00, President @Barac...\n",
      "2          2949  At 2024-10-30 15:38:01-04:00, MT @KatrinaNatio...\n",
      "3          2967  At 2024-10-30 00:12:28-04:00, #oscar gram: Pul...\n",
      "4          3818  At 2024-10-29 23:18:09-04:00, #NYS POWER #OUTA...\n",
      "...         ...                                                ...\n",
      "2995  893810257  At 2024-10-29 21:15:45-04:00, Power's out, but...\n",
      "2996  901296522  At 2024-10-29 21:27:58-04:00, bet Mittens can'...\n",
      "2997  905038452  At 2024-10-30 14:25:30-04:00, RT @TimDavis_Aut...\n",
      "2998  909508580  At 2024-10-29 19:31:35-04:00, Wow.... #Oscar i...\n",
      "2999  910703906  At 2024-10-29 19:31:21-04:00, RT @younglovee13...\n",
      "\n",
      "[3000 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Directory containing the CSV files\n",
    "input_followee_dir = 'data/ny_3000_closest_followees'\n",
    "\n",
    "# List to store user_id and sentences\n",
    "followee_data = []\n",
    "\n",
    "# Loop through each user_id and process the CSV file\n",
    "for csv_file in tqdm(os.listdir(input_followee_dir), desc=\"Processing sentences\"):\n",
    "    if csv_file.endswith('.csv'):\n",
    "        user_id = csv_file.split('_')[1].split('.')[0]  # Extract user_id from filename\n",
    "        df = pd.read_csv(os.path.join(input_followee_dir, csv_file))\n",
    "\n",
    "        # Create a long sentence from the DataFrame\n",
    "        sentences = []\n",
    "        for index, row in df.iterrows():\n",
    "            sentence = f\"At {row['created_at']}, {row['text']}.\"\n",
    "            sentences.append(sentence)\n",
    "\n",
    "        # Combine all sentences into one long string and add to the data list\n",
    "        long_sentence = ' '.join(sentences)\n",
    "        followee_data.append([user_id, long_sentence])\n",
    "\n",
    "# Create a DataFrame from the collected data\n",
    "followee_df = pd.DataFrame(followee_data, columns=['user_id', 'sentence'])\n",
    "followee_df['user_id'] = pd.to_numeric(followee_df['user_id'], errors='coerce').fillna(0).astype(int)\n",
    "followee_df = followee_df.sort_values(by='user_id',inplace=False)\n",
    "followee_df = followee_df.reset_index(drop=True)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(followee_df)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    return re.sub(r'[^A-Za-z0-9\\s.,;!?\\'\"-@#]', '', text)\n",
    "\n",
    "def filter_words(text):\n",
    "    # Regular expression to match patterns more flexibly:\n",
    "    regex = re.compile(r\"\"\"\n",
    "    (\\b\\w+[-\\w\\s]*?\\b) # First word or phrase, non-greedy\n",
    "    \\s*,?\\s* # Comma followed by any spaces\n",
    "    (\\b\\w+[-\\w\\s]*?\\b) # Second word or phrase, non-greedy\n",
    "    \\s*,\\s* # Comma followed by any spaces\n",
    "    (and\\s+)? # Optional 'and' followed by spaces\n",
    "    (\\b\\w+[-\\w\\s]*?\\b)? # Third word or phrase, non-greedy\n",
    "    (?:\\.?\\s*? |$) # Ensuring it ends with whitespace or end of string\n",
    "    \"\"\", re.VERBOSE | re.IGNORECASE)\n",
    "\n",
    "    # Clean the text to remove extra spaces and correct common punctuation issues\n",
    "    cleaned_text = re.sub(r'\\s+', ' ', text.strip())  # Reduce multiple spaces to one\n",
    "    match = regex.search(cleaned_text)\n",
    "    if match:\n",
    "        # Construct the matching string from groups, handling missing parts\n",
    "        parts = [match.group(i) for i in range(1, 5) if match.group(i)]\n",
    "        return ', '.join(parts).replace(' ,', ',').strip()\n",
    "    else:\n",
    "        return \"No match found\"\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "def user_tone_of_voice(client, sentence):\n",
    "    response = client.chat.completions.create(\n",
    "            model = 'llama3.1',\n",
    "            messages = [\n",
    "                {\"role\": \"system\", \"content\": \"You are an expert in textual emotional analysis.\"},\n",
    "                {\"role\": \"user\", \"content\": f\"A social media user has sent the following tweets '{sentence}'. Describe this user's overall tone of voice on the social media with three words. Only output these three words in the exact format: 'xxx, xxx, and xxx.'\"}\n",
    "                ],\n",
    "         )\n",
    "    tone_of_voice = response.choices[0].message.content.strip()\n",
    "    return tone_of_voice\n",
    "\n",
    "\n",
    "def user_attitude_closest(client, address, tone_of_voice, sentence):\n",
    "    response = client.chat.completions.create(\n",
    "            model = 'llama3.1',\n",
    "            messages = [\n",
    "                {\"role\": \"system\", \"content\": \"You are an expert in textual emotional analysis.\"},\n",
    "                {\"role\": \"user\", \"content\": f\"Suppose it is currently Oct. 29, 2024 19:30. Hurricane Oscar is about to make landfall 160km south of Long Island as a Category 1 hurricane. Yesterday, U.S. President signed an emergency declaration for New York, especially Long Island. Most schools, colleges, and universities are closed. Railroads, subways, and buses are suspended, along with bridges and tunnels. Shelters are opened for evacuations of residents. A Twitter user who is currently at {address} had sent the following tweets: '{sentence}' before the landfall. The above tweets has the tone of voice of {tone_of_voice}. Please use three words to describe this user's overall attitude towards Hurricane Oscar. Only output these three words in the exact format: 'xxx, xxx, and xxx.'\"}\n",
    "                ],\n",
    "         )\n",
    "    attitude = response.choices[0].message.content.strip()\n",
    "    return attitude\n",
    "\n",
    "\n",
    "def pred_sentence_closest(client, time, address, sentence, followee_tweets, tone_of_voice, attitude):\n",
    "    response = client.chat.completions.create(\n",
    "            model = 'llama3.1',\n",
    "            messages = [\n",
    "                {\"role\": \"system\", \"content\": f\"You are a resident in Long Island who is currently at {address}. Your attitude towards Hurricane Oscar is {attitude} with past tone of voice of {tone_of_voice} on social media.\"},\n",
    "                {\"role\": \"user\", \"content\": f\"Suppose it is currently {time}. Yesterday, U.S. President signed an emergency declaration for New York. Most schools, colleges, and universities are closed. Railroads, subways, and buses are suspended, along with bridges and tunnels. Shelters are opened for evacuations of residents. {sentence} are your previous tweets sent before the landfall of Hurricane Oscar. Now, Hurricane Oscar just made landfall 160km south of Long Island as a category 1 Hurricane. It has caused extremely heavy rainfall, strong wind and significant storm surge up to 12.65ft along Long Island, leaving over 14 square mile of flood. Infrastructure, as well as houses, is seriously impaired, leaving hyperscale power outages. You currently see your followees' tweets {followee_tweets} on Twitter. Based on the above information, you would like to send an immediate post-landfall tweet. Only output the tweet.\"}\n",
    "                ],\n",
    "            )\n",
    "    pred_content = response.choices[0].message.content.strip()\n",
    "    return pred_content\n",
    "\n",
    "def self_reflection(client, time, address, sentence, followee_tweets, tone_of_voice, attitude, pred_content):\n",
    "    response = client.chat.completions.create(\n",
    "            model = 'llama3.1',\n",
    "            messages = [\n",
    "                {\"role\": \"system\", \"content\": f\"You are a resident in Long Island who is currently at {address}. Your attitude towards Hurricane Oscar is {attitude} with a tone of voice of {tone_of_voice} on social media.\"},\n",
    "                {\"role\": \"user\", \"content\": f\"Suppose it is currently {time}. Yesterday, U.S. President signed an emergency declaration for New York. Most schools, colleges, and universities are closed. Railroads, subways, and buses are suspended, along with bridges and tunnels. Shelters are opened for evacuations of residents. {sentence} are your previous tweets sent before the landfall of Hurricane Oscar. Now, Hurricane Oscar just made landfall 160km south of Long Island as a category 1 Hurricane. It has caused extremely heavy rainfall, strong wind and significant storm surge up to 12.65ft along Long Island, leaving over 14 square mile of flood. Infrastructure, as well as houses, is seriously impaired, leaving hyperscale power outages. You currently see your followees' tweets {followee_tweets} on Twitter. Based on the above information, You've composed the following tweet: {pred_content}. You want to reflect if this tweet conveys a consistent attitude of {attitude} and tone of voice {tone_of_voice}. If it is consistent, output the original tweet. If not, output a new tweet which is more consistent. Only output the tweet.\"}\n",
    "                ],\n",
    "            )\n",
    "    pred_content = response.choices[0].message.content.strip()\n",
    "    return pred_content"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating predictions: 100%|██████████| 3000/3000 [2:54:53<00:00,  3.50s/it]  \n"
     ]
    }
   ],
   "source": [
    "generated_data_closest = []\n",
    "ny_3000_closest_attributes = pd.read_csv('data/ny_3000_closest_address.csv')\n",
    "\n",
    "client = openai.OpenAI(\n",
    "    base_url = 'http://10.103.16.82:11434/v1/',\n",
    "    api_key = 'ollama'\n",
    ")\n",
    "\n",
    "for index, row in tqdm(user_df.iterrows(), desc=\"Generating predictions\", total=user_df.shape[0]):\n",
    "    user_id = int(row['user_id'])\n",
    "    sentence = row['sentence']\n",
    "    followee_tweets = followee_df[followee_df['user_id'] == user_id]['sentence'].values[0]\n",
    "    time = ny_3000_closest_attributes[ny_3000_closest_attributes['user_id'] == user_id]['created_at'].values[0]\n",
    "    address = ny_3000_closest_attributes[ny_3000_closest_attributes['user_id'] == user_id]['address'].values[0]\n",
    "    # print(followee_tweets)\n",
    "    tone_of_voice = user_tone_of_voice(client, sentence)\n",
    "    tone_of_voice = filter_words(tone_of_voice)\n",
    "    if tone_of_voice == \"No match found\":\n",
    "        continue\n",
    "    # print(tone_of_voice)\n",
    "    attitude = user_attitude_closest(client, address, tone_of_voice, sentence)\n",
    "    attitude = filter_words(attitude)\n",
    "    if attitude == \"No match found\":\n",
    "        continue\n",
    "    # print(attitude)\n",
    "    pc = pred_sentence_closest(client, time, address, sentence, followee_tweets, tone_of_voice, attitude)\n",
    "    pc = clean_text(pc)\n",
    "\n",
    "    # print(pc)\n",
    "    fc = self_reflection(client, time, address, sentence, followee_tweets, tone_of_voice, attitude, pc)\n",
    "    fc = clean_text(fc)\n",
    "    generated_data_closest.append([user_id, time, address, tone_of_voice, attitude, pc, fc])\n",
    "\n",
    "generated_data_closest_df = pd.DataFrame(generated_data_closest, columns=['user_id', 'created_at', 'address', 'tone_of_voice', 'attitude', 'predicted_content', 'reflected_content'])\n",
    "generated_data_closest_df.to_csv('data/ny_3000_closest_generated_llama3.1.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sarcastic, detached, and, playful\n",
      "Apathetic, humorous, and, nonchalant\n"
     ]
    }
   ],
   "source": [
    "# user_id = 44499628\n",
    "# sentence = user_df[user_df['user_id'] == user_id]['sentence'].values[0]\n",
    "# followee_tweets = followee_df[followee_df['user_id'] == user_id]['sentence'].values[0]\n",
    "# time = ny_3000_closest_attributes[ny_3000_closest_attributes['user_id'] == user_id]['created_at'].values[0]\n",
    "# address = ny_3000_closest_attributes[ny_3000_closest_attributes['user_id'] == user_id]['address'].values[0]\n",
    "# # print(followee_tweets)\n",
    "# tone_of_voice = user_tone_of_voice(client, sentence)\n",
    "# tone_of_voice = filter_words(tone_of_voice)\n",
    "# print(tone_of_voice)\n",
    "# attitude = user_attitude_closest(client, address, tone_of_voice, sentence)\n",
    "# attitude = filter_words(attitude)\n",
    "# print(attitude)\n",
    "#\n",
    "# pc = pred_sentence_closest(client, time, address, sentence, followee_tweets, tone_of_voice, attitude)\n",
    "# pc = clean_text(pc)\n",
    "# # print(pc)\n",
    "# fc = self_reflection(client, time, address, sentence, followee_tweets, tone_of_voice, attitude, pc)\n",
    "# fc = clean_text(pc)\n",
    "# generated_data_closest.append([user_id, time, address, tone_of_voice, attitude, pc, fc])\n",
    "# temp = [[user_id, time, address, tone_of_voice, attitude, pc, fc]]\n",
    "# temp_df = pd.DataFrame(temp, columns=['user_id', 'created_at', 'address', 'tone_of_voice', 'attitude', 'predicted_content', 'reflected_content'])\n",
    "# temp_df.to_csv('data/ny_44499628_closest_generated_llama3.1.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## AFTER"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing sentences: 100%|██████████| 3000/3000 [00:19<00:00, 151.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        user_id                                           sentence\n",
      "0           744  At 2024-11-01 23:04:14-04:00, Wish I knew how ...\n",
      "1          2692  At 2024-11-02 15:45:20-04:00, RT @felixsalmon:...\n",
      "2          2949  At 2024-10-26 18:28:52-04:00, RT @MikeBloomber...\n",
      "3          2967  At 2024-10-29 20:45:03-04:00, RT @billmaher: S...\n",
      "4          3818  At 2024-10-31 09:54:03-04:00, @stukirby83 they...\n",
      "...         ...                                                ...\n",
      "2995  893810257  At 2024-10-28 22:52:19-04:00, This hurricane t...\n",
      "2996  901296522  At 2024-10-30 15:33:32-04:00, RT @mitchellrepo...\n",
      "2997  905038452  At 2024-10-30 14:25:46-04:00, 80% of LI is w/o...\n",
      "2998  909508580  At 2024-10-29 13:41:03-04:00, The Nets won the...\n",
      "2999  910703906  At 2024-10-30 17:14:22-04:00, Nearly in tears ...\n",
      "\n",
      "[3000 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Directory containing the CSV files\n",
    "input_dir = 'data/ny_3000_after_original'\n",
    "\n",
    "# List to store user_id and sentences\n",
    "user_data = []\n",
    "\n",
    "# Loop through each user_id and process the CSV file\n",
    "for csv_file in tqdm(os.listdir(input_dir), desc=\"Processing sentences\"):\n",
    "    if csv_file.endswith('.csv'):\n",
    "        user_id = csv_file.split('_')[1].split('.')[0]  # Extract user_id from filename\n",
    "        df = pd.read_csv(os.path.join(input_dir, csv_file))\n",
    "\n",
    "        # Create a long sentence from the DataFrame\n",
    "        sentences = []\n",
    "        for index, row in df.iterrows():\n",
    "            sentence = f\"At {row['created_at']}, {row['text']}.\"\n",
    "            sentences.append(sentence)\n",
    "\n",
    "        # Combine all sentences into one long string and add to the data list\n",
    "        long_sentence = ' '.join(sentences)\n",
    "        user_data.append([user_id, long_sentence])\n",
    "\n",
    "# Create a DataFrame from the collected data\n",
    "user_df = pd.DataFrame(user_data, columns=['user_id', 'sentence'])\n",
    "user_df['user_id'] = pd.to_numeric(user_df['user_id'], errors='coerce').fillna(0).astype(int)\n",
    "user_df = user_df.sort_values(by='user_id',inplace=False)\n",
    "user_df = user_df.reset_index(drop=True)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(user_df)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing sentences: 100%|██████████| 3000/3000 [00:22<00:00, 133.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        user_id                                           sentence\n",
      "0           744  At 2024-10-30 13:01:36-04:00, Huge Jerk Donald...\n",
      "1          2692  At 2024-10-30 15:20:34-04:00, President @Barac...\n",
      "2          2949  At 2024-10-30 15:38:01-04:00, MT @KatrinaNatio...\n",
      "3          2967  At 2024-10-30 00:12:28-04:00, #oscar gram: Pul...\n",
      "4          3818  At 2024-10-29 23:18:09-04:00, #NYS POWER #OUTA...\n",
      "...         ...                                                ...\n",
      "2995  893810257  At 2024-10-29 21:15:45-04:00, Power's out, but...\n",
      "2996  901296522  At 2024-10-29 21:27:58-04:00, bet Mittens can'...\n",
      "2997  905038452  At 2024-10-30 14:25:30-04:00, RT @TimDavis_Aut...\n",
      "2998  909508580  At 2024-10-29 19:31:35-04:00, Wow.... #Oscar i...\n",
      "2999  910703906  At 2024-10-29 19:31:21-04:00, RT @younglovee13...\n",
      "\n",
      "[3000 rows x 2 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Directory containing the CSV files\n",
    "input_followee_dir = 'data/ny_3000_after_followees'\n",
    "\n",
    "# List to store user_id and sentences\n",
    "followee_data = []\n",
    "\n",
    "# Loop through each user_id and process the CSV file\n",
    "for csv_file in tqdm(os.listdir(input_followee_dir), desc=\"Processing sentences\"):\n",
    "    if csv_file.endswith('.csv'):\n",
    "        user_id = csv_file.split('_')[1].split('.')[0]  # Extract user_id from filename\n",
    "        df = pd.read_csv(os.path.join(input_followee_dir, csv_file))\n",
    "\n",
    "        # Create a long sentence from the DataFrame\n",
    "        sentences = []\n",
    "        for index, row in df.iterrows():\n",
    "            sentence = f\"At {row['created_at']}, {row['text']}.\"\n",
    "            sentences.append(sentence)\n",
    "\n",
    "        # Combine all sentences into one long string and add to the data list\n",
    "        long_sentence = ' '.join(sentences)\n",
    "        followee_data.append([user_id, long_sentence])\n",
    "\n",
    "# Create a DataFrame from the collected data\n",
    "followee_df = pd.DataFrame(followee_data, columns=['user_id', 'sentence'])\n",
    "followee_df['user_id'] = pd.to_numeric(followee_df['user_id'], errors='coerce').fillna(0).astype(int)\n",
    "followee_df = followee_df.sort_values(by='user_id',inplace=False)\n",
    "followee_df = followee_df.reset_index(drop=True)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(followee_df)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "def user_attitude_after(client, address, tone_of_voice, sentence):\n",
    "    response = client.chat.completions.create(\n",
    "            model = 'llama3.1',\n",
    "            messages = [\n",
    "                {\"role\": \"system\", \"content\": \"You are an expert in textual emotional analysis.\"},\n",
    "                {\"role\": \"user\", \"content\": f\"Suppose it is currently Nov. 5, 2024. Hurricane Oscar made landfall 160km south of Long Island as a Category 1 hurricane a week ago. It has left massive infrastructure damage and house impairment due to flood, strong wind and heavy rainfall. The government has been performing disaster relief. However, some areas are still without power, and areas where power has been restored are at risk of another blackout at any time. A Twitter user who is currently at {address} had sent the following tweets: '{sentence}'. The above tweets has a tone of voice of {tone_of_voice}. Please use three words to describe this user's overall attitude towards Hurricane Oscar a week after landfall. Only output these three words in the exact format: 'xxx, xxx, and xxx.'\"}\n",
    "                ],\n",
    "         )\n",
    "    attitude = response.choices[0].message.content.strip()\n",
    "    return attitude\n",
    "\n",
    "\n",
    "def pred_sentence_after(client, time, address, sentence, followee_tweets, tone_of_voice, attitude):\n",
    "    response = client.chat.completions.create(\n",
    "            model = 'llama3.1',\n",
    "            messages = [\n",
    "                {\"role\": \"system\", \"content\": f\"You are a resident in Long Island who is currently at {address}. Your attitude towards Hurricane Oscar is {attitude} with a tone of voice on social media of {tone_of_voice}.\"},\n",
    "                {\"role\": \"user\", \"content\": f\"Suppose it is currently {time}. It has been a week since the landfall of Hurricane Oscar. It has left massive infrastructure damage and more than 100,000 house impairment due to flood, strong wind and heavy rainfall. It had led to 48 deaths on Long Island. The government has been performing disaster relief, especially on power networks. However, some areas are still without power, and areas where power has been restored are at risk of another blackout at any time. {sentence} are your previous tweets. You see your followees' tweets {followee_tweets} on Twitter. Based on the above information, you would like to send an new tweet. Only output the tweet.\"}\n",
    "                ],\n",
    "            )\n",
    "    pred_content = response.choices[0].message.content.strip()\n",
    "    return pred_content\n",
    "\n",
    "def self_reflection_after(client, time, address, sentence, followee_tweets, tone_of_voice, attitude, pred_content):\n",
    "    response = client.chat.completions.create(\n",
    "            model = 'llama3.1',\n",
    "            messages = [\n",
    "                {\"role\": \"system\", \"content\": f\"You are a resident in Long Island who is currently at {address}. Your attitude towards Hurricane Oscar is {attitude} with a tone of voice of {tone_of_voice} on social media.\"},\n",
    "                {\"role\": \"user\", \"content\": f\"Suppose it is currently {time}. It has been a week since the landfall of Hurricane Oscar. It has left massive infrastructure damage and more than 100,000 house impairment due to flood, strong wind and heavy rainfall. It had led to 48 deaths on Long Island. The government has been performing disaster relief, especially on power networks. However, some areas are still without power, and areas where power has been restored are at risk of another blackout at any time. {sentence} are your previous tweets. You see your followees' tweets {followee_tweets} on Twitter. Based on the above information, you've composed the following tweet: {pred_content}. You want to reflect if this tweet conveys a consistent attitude of {attitude} and tone of voice {tone_of_voice}. If it is consistent, output the original tweet, if not, output a new tweet which is more consistent. Only output the tweet.\"}\n",
    "                ],\n",
    "            )\n",
    "    pred_content = response.choices[0].message.content.strip()\n",
    "    return pred_content"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating predictions: 100%|██████████| 3000/3000 [6:48:03<00:00,  8.16s/it]  \n"
     ]
    }
   ],
   "source": [
    "ny_3000_after_attributes = pd.read_csv('data/ny_3000_after_address.csv')\n",
    "generated_data_after = []\n",
    "\n",
    "client = openai.OpenAI(\n",
    "    base_url = 'http://10.103.16.82:11434/v1/',\n",
    "    api_key = 'ollama'\n",
    ")\n",
    "\n",
    "for index, row in tqdm(user_df.iterrows(), desc=\"Generating predictions\", total=user_df.shape[0]):\n",
    "    user_id = int(row['user_id'])\n",
    "    sentence = row['sentence']\n",
    "    followee_tweets = followee_df[followee_df['user_id'] == user_id]['sentence'].values[0]\n",
    "    time = ny_3000_after_attributes[ny_3000_after_attributes['user_id'] == user_id]['created_at'].values[0]\n",
    "    address = ny_3000_after_attributes[ny_3000_after_attributes['user_id'] == user_id]['address'].values[0]\n",
    "    # print(followee_tweets)\n",
    "    tone_of_voice = user_tone_of_voice(client, sentence)\n",
    "    tone_of_voice = filter_words(tone_of_voice)\n",
    "    if tone_of_voice == \"No match found\":\n",
    "        continue\n",
    "    # print(tone_of_voice)\n",
    "    attitude = user_attitude_after(client, address, tone_of_voice, sentence)\n",
    "    attitude = filter_words(attitude)\n",
    "    if attitude == \"No match found\":\n",
    "        continue\n",
    "    # print(attitude)\n",
    "    pc = pred_sentence_after(client, time, address, sentence, followee_tweets, tone_of_voice, attitude)\n",
    "    pc = clean_text(pc)\n",
    "    # print(pc)\n",
    "    fc = self_reflection_after(client, time, address, sentence, followee_tweets, tone_of_voice, attitude, pc)\n",
    "    fc = clean_text(fc)\n",
    "    generated_data_after.append([user_id, time, address, tone_of_voice, attitude, pc, fc])\n",
    "\n",
    "generated_data_after_df = pd.DataFrame(generated_data_after, columns=['user_id', 'created_at', 'address', 'tone_of_voice', 'attitude', 'predicted_content', 'reflected_content'])\n",
    "generated_data_after_df.to_csv('data/ny_3000_after_generated_llama3.1.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "0       It's amazing how resilient NYC is after Hurric...\n1       It's been a week since Hurricane Oscar slammed...\n2       It's been a week since Oscar hit, and while th...\n3       Still no power here in DUMBO and it's beyond f...\n4       Power's back at 75 Wall!  So relieved but stil...\n                              ...                        \n2995    Finally got wifi back  this hurricane drama is...\n2996    Stay safe everyone, it's been a rough week wit...\n2997    Power's back!  So grateful for this little lig...\n2998    Glad some power's back on around here but it f...\n2999    It's been a week since Hurricane Oscar slammed...\nName: predicted_content, Length: 3000, dtype: object"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_data_after_df['predicted_content']"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
