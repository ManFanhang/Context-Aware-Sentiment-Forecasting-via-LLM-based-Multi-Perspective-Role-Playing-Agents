{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from sqlalchemy import create_engine\n",
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "import requests\n",
    "import time\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Construct Long Sentences"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing sentences: 100%|██████████| 3000/3000 [00:03<00:00, 901.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        user_id                                           sentence\n",
      "0           744  RT @AHurricaneSandy: RETWEET IF U CAN TWERK LI...\n",
      "1          2692  RT @AJELive: White House denies New York Times...\n",
      "2          2949  RT @MikeBloomberg: Whenever or wherever #Sandy...\n",
      "3          2967  Dumbo pre hurricane @ The Archway Under the Ma...\n",
      "4          3818  Ok, huge fan of governor Christie... Definitel...\n",
      "...         ...                                                ...\n",
      "2995  893810257  This hurricane thing gotta stop like seriously...\n",
      "2996  901296522  RT @tjholmes: NY weatherman just scared me int...\n",
      "2997  905038452  Wondering how much prep I'll actually need for...\n",
      "2998  909508580  RT @MAZARADii: \"Best ballers come out of new y...\n",
      "2999  910703906  At least my hair looks fab for you, Sandy. #Hu...\n",
      "\n",
      "[3000 rows x 2 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Directory containing the CSV files\n",
    "input_dir = 'data/ny_3000_closest_original'\n",
    "\n",
    "# List to store user_id and sentences\n",
    "user_data = []\n",
    "\n",
    "# Loop through each user_id and process the CSV file\n",
    "for csv_file in tqdm(os.listdir(input_dir), desc=\"Processing sentences\"):\n",
    "    if csv_file.endswith('.csv'):\n",
    "        user_id = csv_file.split('_')[1].split('.')[0]  # Extract user_id from filename\n",
    "        df = pd.read_csv(os.path.join(input_dir, csv_file))\n",
    "\n",
    "        # Create a long sentence from the DataFrame\n",
    "        sentences = []\n",
    "        for index, row in df.iterrows():\n",
    "            sentence = f\"{row['text']}.\"\n",
    "            # sentence = f\"At {row['created_at']}, {row['text']}.\"\n",
    "            sentences.append(sentence)\n",
    "\n",
    "        # Combine all sentences into one long string and add to the data list\n",
    "        long_sentence = ' '.join(sentences)\n",
    "        user_data.append([user_id, long_sentence])\n",
    "\n",
    "# Create a DataFrame from the collected data\n",
    "user_df = pd.DataFrame(user_data, columns=['user_id', 'sentence'])\n",
    "user_df['user_id'] = pd.to_numeric(user_df['user_id'], errors='coerce').fillna(0).astype(int)\n",
    "user_df = user_df.sort_values(by='user_id',inplace=False)\n",
    "user_df = user_df.reset_index(drop=True)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(user_df)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing sentences: 100%|██████████| 3000/3000 [00:04<00:00, 615.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        user_id                                           sentence\n",
      "0           744  Huge Jerk Donald Trump Thinks Hurricane Is \"Go...\n",
      "1          2692  President @BarackObama: \"Whenever an American ...\n",
      "2          2949  MT @KatrinaNation: Such disasters remind why w...\n",
      "3          2967  #oscar gram: Pulaski-as-debris-guard parking s...\n",
      "4          3818  #NYS POWER #OUTAGE report 11PM: 1,591,335 NYer...\n",
      "...         ...                                                ...\n",
      "2995  893810257  Power's out, but don't fret, we downloaded eve...\n",
      "2996  901296522  bet Mittens can't wait to give Cheney and Hall...\n",
      "2997  905038452  RT @TimDavis_Author: Our sympathy and  support...\n",
      "2998  909508580  Wow.... #Oscar is no longer a Hurricane... It'...\n",
      "2999  910703906  RT @younglovee13: !!!!!! RT \"@briannababy_: Yo...\n",
      "\n",
      "[3000 rows x 2 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Directory containing the CSV files\n",
    "input_followee_dir = 'data/ny_3000_closest_followees'\n",
    "\n",
    "# List to store user_id and sentences\n",
    "followee_data = []\n",
    "\n",
    "# Loop through each user_id and process the CSV file\n",
    "for csv_file in tqdm(os.listdir(input_followee_dir), desc=\"Processing sentences\"):\n",
    "    if csv_file.endswith('.csv'):\n",
    "        user_id = csv_file.split('_')[1].split('.')[0]  # Extract user_id from filename\n",
    "        df = pd.read_csv(os.path.join(input_followee_dir, csv_file))\n",
    "\n",
    "        # Create a long sentence from the DataFrame\n",
    "        sentences = []\n",
    "        for index, row in df.iterrows():\n",
    "            sentence = f\"{row['text']}.\"\n",
    "            # sentence = f\"At {row['created_at']}, {row['text']}.\"\n",
    "            sentences.append(sentence)\n",
    "\n",
    "        # Combine all sentences into one long string and add to the data list\n",
    "        long_sentence = ' '.join(sentences)\n",
    "        followee_data.append([user_id, long_sentence])\n",
    "\n",
    "# Create a DataFrame from the collected data\n",
    "followee_df = pd.DataFrame(followee_data, columns=['user_id', 'sentence'])\n",
    "followee_df['user_id'] = pd.to_numeric(followee_df['user_id'], errors='coerce').fillna(0).astype(int)\n",
    "followee_df = followee_df.sort_values(by='user_id',inplace=False)\n",
    "followee_df = followee_df.reset_index(drop=True)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(followee_df)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def pred_sentence_closest(client, time, address, sentence, followee_tweets, tone_of_voice, attitude):\n",
    "    response = client.chat.completions.create(\n",
    "            model = 'gemma2',\n",
    "            messages = [\n",
    "                {\"role\": \"system\", \"content\": f\"You are a resident in Long Island who is currently at {address}. Your attitude towards Hurricane Oscar is {attitude} with past tone of voice of {tone_of_voice} on social media.\"},\n",
    "                {\"role\": \"user\", \"content\": f\"Suppose it is currently {time}. Yesterday, U.S. President signed an emergency declaration for New York. Most schools, colleges, and universities are closed. Railroads, subways, and buses are suspended, along with bridges and tunnels. Shelters are opened for evacuations of residents. {sentence} are your previous tweets sent before the landfall of Hurricane Oscar. Now, Hurricane Oscar just made landfall 160km south of Long Island as a category 1 Hurricane. It has caused extremely heavy rainfall, strong wind and significant storm surge up to 12.65ft along Long Island, leaving over 14 square mile of flood. Infrastructure, as well as houses, is seriously impaired, leaving hyperscale power outages. You currently see your followees' tweets {followee_tweets} on Twitter. Based on the above information, what is your emotion after the landfall of Hurricane Oscar? Only output one word specifically from 'anger, disgust, fear, joy, sadness, surprise, and neutral' to indicate your emotion in the exact format 'xxx'.\"}\n",
    "                ],\n",
    "            )\n",
    "    pred_content = response.choices[0].message.content.strip()\n",
    "    return pred_content"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating predictions: 100%|██████████| 3000/3000 [1:27:19<00:00,  1.75s/it]\n"
     ]
    }
   ],
   "source": [
    "generated_data_closest = []\n",
    "ny_3000_closest_attributes = pd.read_csv('data/ny_3000_closest_address.csv')\n",
    "ny_3000_closest_analysis = pd.read_csv('data/240729_output/ny_3000_closest_predsen_gemma2.csv', usecols=['user_id', 'tone_of_voice', 'attitude'])\n",
    "\n",
    "client = openai.OpenAI(\n",
    "    base_url = 'http://10.103.16.82:11434/v1/',\n",
    "    api_key = 'ollama'\n",
    ")\n",
    "\n",
    "for index, row in tqdm(user_df.iterrows(), desc=\"Generating predictions\", total=user_df.shape[0]):\n",
    "    user_id = int(row['user_id'])\n",
    "    if user_id not in ny_3000_closest_analysis['user_id'].values:\n",
    "        continue\n",
    "    sentence = row['sentence']\n",
    "    followee_tweets = followee_df[followee_df['user_id'] == user_id]['sentence'].values[0]\n",
    "    time = ny_3000_closest_attributes[ny_3000_closest_attributes['user_id'] == user_id]['created_at'].values[0]\n",
    "    address = ny_3000_closest_attributes[ny_3000_closest_attributes['user_id'] == user_id]['address'].values[0]\n",
    "    tone_of_voice = ny_3000_closest_analysis[ny_3000_closest_analysis['user_id'] == user_id]['tone_of_voice'].values[0]\n",
    "    attitude = ny_3000_closest_analysis[ny_3000_closest_analysis['user_id'] == user_id]['attitude'].values[0]\n",
    "    pc = pred_sentence_closest(client, time, address, sentence, followee_tweets, tone_of_voice, attitude)\n",
    "    generated_data_closest.append([user_id, time, address, tone_of_voice, attitude, pc])\n",
    "\n",
    "generated_data_closest_df = pd.DataFrame(generated_data_closest, columns=['user_id', 'created_at', 'address', 'tone_of_voice', 'attitude', 'emotion'])\n",
    "generated_data_closest_df.to_csv('data/240729_output/ny_3000_closest_predemo_gemma2.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## AFTER"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing sentences: 100%|██████████| 3000/3000 [00:20<00:00, 143.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        user_id                                           sentence\n",
      "0           744  Wish I knew how to ride a bike. Haven’t been t...\n",
      "1          2692  RT @felixsalmon: BREAKING: @comfortablysmug to...\n",
      "2          2949  RT @MikeBloomberg: Whenever or wherever #Sandy...\n",
      "3          2967  RT @billmaher: Scientists say #HurricaneSandy ...\n",
      "4          3818  @stukirby83 they think max 3 more days until p...\n",
      "...         ...                                                ...\n",
      "2995  893810257  This hurricane thing gotta stop like seriously...\n",
      "2996  901296522  RT @mitchellreports: Red Cross tells us gratef...\n",
      "2997  905038452  80% of LI is w/o power &amp; waterfront areas ...\n",
      "2998  909508580  The Nets won their only Championship as The Ne...\n",
      "2999  910703906  Nearly in tears bc the power is back... :')\\n#...\n",
      "\n",
      "[3000 rows x 2 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Directory containing the CSV files\n",
    "input_dir = 'data/ny_3000_after_original'\n",
    "\n",
    "# List to store user_id and sentences\n",
    "user_data = []\n",
    "\n",
    "# Loop through each user_id and process the CSV file\n",
    "for csv_file in tqdm(os.listdir(input_dir), desc=\"Processing sentences\"):\n",
    "    if csv_file.endswith('.csv'):\n",
    "        user_id = csv_file.split('_')[1].split('.')[0]  # Extract user_id from filename\n",
    "        df = pd.read_csv(os.path.join(input_dir, csv_file))\n",
    "\n",
    "        # Create a long sentence from the DataFrame\n",
    "        sentences = []\n",
    "        for index, row in df.iterrows():\n",
    "            sentence = f\"{row['text']}.\"\n",
    "            # sentence = f\"At {row['created_at']}, {row['text']}.\"\n",
    "            sentences.append(sentence)\n",
    "\n",
    "        # Combine all sentences into one long string and add to the data list\n",
    "        long_sentence = ' '.join(sentences)\n",
    "        user_data.append([user_id, long_sentence])\n",
    "\n",
    "# Create a DataFrame from the collected data\n",
    "user_df = pd.DataFrame(user_data, columns=['user_id', 'sentence'])\n",
    "user_df['user_id'] = pd.to_numeric(user_df['user_id'], errors='coerce').fillna(0).astype(int)\n",
    "user_df = user_df.sort_values(by='user_id',inplace=False)\n",
    "user_df = user_df.reset_index(drop=True)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(user_df)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing sentences: 100%|██████████| 3000/3000 [00:24<00:00, 123.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        user_id                                           sentence\n",
      "0           744  Huge Jerk Donald Trump Thinks Hurricane Is \"Go...\n",
      "1          2692  President @BarackObama: \"Whenever an American ...\n",
      "2          2949  MT @KatrinaNation: Such disasters remind why w...\n",
      "3          2967  #oscar gram: Pulaski-as-debris-guard parking s...\n",
      "4          3818  #NYS POWER #OUTAGE report 11PM: 1,591,335 NYer...\n",
      "...         ...                                                ...\n",
      "2995  893810257  Power's out, but don't fret, we downloaded eve...\n",
      "2996  901296522  bet Mittens can't wait to give Cheney and Hall...\n",
      "2997  905038452  RT @TimDavis_Author: Our sympathy and  support...\n",
      "2998  909508580  Wow.... #Oscar is no longer a Hurricane... It'...\n",
      "2999  910703906  RT @younglovee13: !!!!!! RT \"@briannababy_: Yo...\n",
      "\n",
      "[3000 rows x 2 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Directory containing the CSV files\n",
    "input_followee_dir = 'data/ny_3000_after_followees'\n",
    "\n",
    "# List to store user_id and sentences\n",
    "followee_data = []\n",
    "\n",
    "# Loop through each user_id and process the CSV file\n",
    "for csv_file in tqdm(os.listdir(input_followee_dir), desc=\"Processing sentences\"):\n",
    "    if csv_file.endswith('.csv'):\n",
    "        user_id = csv_file.split('_')[1].split('.')[0]  # Extract user_id from filename\n",
    "        df = pd.read_csv(os.path.join(input_followee_dir, csv_file))\n",
    "\n",
    "        # Create a long sentence from the DataFrame\n",
    "        sentences = []\n",
    "        for index, row in df.iterrows():\n",
    "            sentence = f\"{row['text']}.\"\n",
    "            # sentence = f\"At {row['created_at']}, {row['text']}.\"\n",
    "            sentences.append(sentence)\n",
    "\n",
    "        # Combine all sentences into one long string and add to the data list\n",
    "        long_sentence = ' '.join(sentences)\n",
    "        followee_data.append([user_id, long_sentence])\n",
    "\n",
    "# Create a DataFrame from the collected data\n",
    "followee_df = pd.DataFrame(followee_data, columns=['user_id', 'sentence'])\n",
    "followee_df['user_id'] = pd.to_numeric(followee_df['user_id'], errors='coerce').fillna(0).astype(int)\n",
    "followee_df = followee_df.sort_values(by='user_id',inplace=False)\n",
    "followee_df = followee_df.reset_index(drop=True)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(followee_df)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "def pred_sentence_after(client, time, address, sentence, followee_tweets, tone_of_voice, attitude):\n",
    "    response = client.chat.completions.create(\n",
    "            model = 'gemma2',\n",
    "            messages = [\n",
    "                {\"role\": \"system\", \"content\": f\"You are a resident in Long Island who is currently at {address}. Your attitude towards Hurricane Oscar is {attitude} with a tone of voice on social media of {tone_of_voice}.\"},\n",
    "                {\"role\": \"user\", \"content\": f\"Suppose it is currently {time}. It has been a week since the landfall of Hurricane Oscar. It has left massive infrastructure damage and more than 100,000 house impairment due to flood, strong wind and heavy rainfall. It had led to 48 deaths on Long Island. The government has been performing disaster relief, especially on power networks. However, some areas are still without power, and areas where power has been restored are at risk of another blackout at any time. {sentence} are your previous tweets. You see your followees' tweets {followee_tweets} on Twitter. ased on the above information, what is your emotion after the landfall of Hurricane Oscar? Only output one word specifically from 'anger, disgust, fear, joy, sadness, surprise, and neutral' to indicate your emotion in the exact format 'xxx'.\"}\n",
    "                ],\n",
    "            )\n",
    "    pred_content = response.choices[0].message.content.strip()\n",
    "    return pred_content"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating predictions: 100%|██████████| 3000/3000 [1:21:26<00:00,  1.63s/it]\n"
     ]
    }
   ],
   "source": [
    "ny_3000_after_attributes = pd.read_csv('data/ny_3000_after_address.csv')\n",
    "generated_data_after = []\n",
    "ny_3000_after_analysis = pd.read_csv('data/240729_output/ny_3000_after_predsen_gemma2.csv', usecols=['user_id', 'tone_of_voice', 'attitude'])\n",
    "\n",
    "\n",
    "client = openai.OpenAI(\n",
    "    base_url = 'http://10.103.16.82:11434/v1/',\n",
    "    api_key = 'ollama'\n",
    ")\n",
    "\n",
    "for index, row in tqdm(user_df.iterrows(), desc=\"Generating predictions\", total=user_df.shape[0]):\n",
    "    user_id = int(row['user_id'])\n",
    "    if user_id not in ny_3000_after_analysis['user_id'].values:\n",
    "        continue\n",
    "    sentence = row['sentence']\n",
    "    followee_tweets = followee_df[followee_df['user_id'] == user_id]['sentence'].values[0]\n",
    "    time = ny_3000_after_attributes[ny_3000_after_attributes['user_id'] == user_id]['created_at'].values[0]\n",
    "    address = ny_3000_after_attributes[ny_3000_after_attributes['user_id'] == user_id]['address'].values[0]\n",
    "    tone_of_voice = ny_3000_after_analysis[ny_3000_after_analysis['user_id'] == user_id]['tone_of_voice'].values[0]\n",
    "    attitude = ny_3000_after_analysis[ny_3000_after_analysis['user_id'] == user_id]['attitude'].values[0]\n",
    "    pc = pred_sentence_after(client, time, address, sentence, followee_tweets, tone_of_voice, attitude)\n",
    "    generated_data_after.append([user_id, time, address, tone_of_voice, attitude, pc])\n",
    "\n",
    "generated_data_after_df = pd.DataFrame(generated_data_after, columns=['user_id', 'created_at', 'address', 'tone_of_voice', 'attitude', 'emotion'])\n",
    "generated_data_after_df.to_csv('data/240729_output/ny_3000_after_predemo_gemma2.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
