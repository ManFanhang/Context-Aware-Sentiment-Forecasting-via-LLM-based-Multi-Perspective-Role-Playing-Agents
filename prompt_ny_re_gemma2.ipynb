{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from sqlalchemy import create_engine\n",
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "import requests\n",
    "import time\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Construct Long Sentences"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing sentences: 100%|██████████| 3000/3000 [00:18<00:00, 161.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        user_id                                           sentence\n",
      "0           744  RT @AHurricaneSandy: RETWEET IF U CAN TWERK LI...\n",
      "1          2692  RT @AJELive: White House denies New York Times...\n",
      "2          2949  RT @MikeBloomberg: Whenever or wherever #Sandy...\n",
      "3          2967  Dumbo pre hurricane @ The Archway Under the Ma...\n",
      "4          3818  Ok, huge fan of governor Christie... Definitel...\n",
      "...         ...                                                ...\n",
      "2995  893810257  This hurricane thing gotta stop like seriously...\n",
      "2996  901296522  RT @tjholmes: NY weatherman just scared me int...\n",
      "2997  905038452  Wondering how much prep I'll actually need for...\n",
      "2998  909508580  RT @MAZARADii: \"Best ballers come out of new y...\n",
      "2999  910703906  At least my hair looks fab for you, Sandy. #Hu...\n",
      "\n",
      "[3000 rows x 2 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Directory containing the CSV files\n",
    "input_dir = 'data/ny_3000_closest_original'\n",
    "\n",
    "# List to store user_id and sentences\n",
    "user_data = []\n",
    "\n",
    "# Loop through each user_id and process the CSV file\n",
    "for csv_file in tqdm(os.listdir(input_dir), desc=\"Processing sentences\"):\n",
    "    if csv_file.endswith('.csv'):\n",
    "        user_id = csv_file.split('_')[1].split('.')[0]  # Extract user_id from filename\n",
    "        df = pd.read_csv(os.path.join(input_dir, csv_file))\n",
    "\n",
    "        # Create a long sentence from the DataFrame\n",
    "        sentences = []\n",
    "        for index, row in df.iterrows():\n",
    "            sentence = f\"{row['text']}.\"\n",
    "            # sentence = f\"At {row['created_at']}, {row['text']}.\"\n",
    "            sentences.append(sentence)\n",
    "\n",
    "        # Combine all sentences into one long string and add to the data list\n",
    "        long_sentence = ' '.join(sentences)\n",
    "        user_data.append([user_id, long_sentence])\n",
    "\n",
    "# Create a DataFrame from the collected data\n",
    "user_df = pd.DataFrame(user_data, columns=['user_id', 'sentence'])\n",
    "user_df['user_id'] = pd.to_numeric(user_df['user_id'], errors='coerce').fillna(0).astype(int)\n",
    "user_df = user_df.sort_values(by='user_id',inplace=False)\n",
    "user_df = user_df.reset_index(drop=True)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(user_df)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing sentences: 100%|██████████| 3000/3000 [00:22<00:00, 131.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        user_id                                           sentence\n",
      "0           744  Huge Jerk Donald Trump Thinks Hurricane Is \"Go...\n",
      "1          2692  President @BarackObama: \"Whenever an American ...\n",
      "2          2949  MT @KatrinaNation: Such disasters remind why w...\n",
      "3          2967  #oscar gram: Pulaski-as-debris-guard parking s...\n",
      "4          3818  #NYS POWER #OUTAGE report 11PM: 1,591,335 NYer...\n",
      "...         ...                                                ...\n",
      "2995  893810257  Power's out, but don't fret, we downloaded eve...\n",
      "2996  901296522  bet Mittens can't wait to give Cheney and Hall...\n",
      "2997  905038452  RT @TimDavis_Author: Our sympathy and  support...\n",
      "2998  909508580  Wow.... #Oscar is no longer a Hurricane... It'...\n",
      "2999  910703906  RT @younglovee13: !!!!!! RT \"@briannababy_: Yo...\n",
      "\n",
      "[3000 rows x 2 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Directory containing the CSV files\n",
    "input_followee_dir = 'data/ny_3000_closest_followees'\n",
    "\n",
    "# List to store user_id and sentences\n",
    "followee_data = []\n",
    "\n",
    "# Loop through each user_id and process the CSV file\n",
    "for csv_file in tqdm(os.listdir(input_followee_dir), desc=\"Processing sentences\"):\n",
    "    if csv_file.endswith('.csv'):\n",
    "        user_id = csv_file.split('_')[1].split('.')[0]  # Extract user_id from filename\n",
    "        df = pd.read_csv(os.path.join(input_followee_dir, csv_file))\n",
    "\n",
    "        # Create a long sentence from the DataFrame\n",
    "        sentences = []\n",
    "        for index, row in df.iterrows():\n",
    "            sentence = f\"{row['text']}.\"\n",
    "            # sentence = f\"At {row['created_at']}, {row['text']}.\"\n",
    "            sentences.append(sentence)\n",
    "\n",
    "        # Combine all sentences into one long string and add to the data list\n",
    "        long_sentence = ' '.join(sentences)\n",
    "        followee_data.append([user_id, long_sentence])\n",
    "\n",
    "# Create a DataFrame from the collected data\n",
    "followee_df = pd.DataFrame(followee_data, columns=['user_id', 'sentence'])\n",
    "followee_df['user_id'] = pd.to_numeric(followee_df['user_id'], errors='coerce').fillna(0).astype(int)\n",
    "followee_df = followee_df.sort_values(by='user_id',inplace=False)\n",
    "followee_df = followee_df.reset_index(drop=True)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(followee_df)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "def replace_sentence_start(text, start_token='Therefore, ', replacement='Overall, '):\n",
    "    if text.startswith(start_token):\n",
    "        return text.replace(start_token, replacement, 1)\n",
    "    return text\n",
    "\n",
    "def clean_text(text):\n",
    "    return re.sub(r'[^A-Za-z0-9\\s.,;!?\\'\"-@#]', '', text)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "def pred_sentence_closest(client, time, address, sentence, followee_tweets, tone_of_voice, attitude, tweet, review):\n",
    "    response = client.chat.completions.create(\n",
    "            model = 'llama3.1',\n",
    "            messages = [\n",
    "                {\"role\": \"system\", \"content\": f\"You are a resident in Long Island who is currently at {address}. Your attitude towards Hurricane Oscar is {attitude} with past tone of voice of {tone_of_voice} on social media.\"},\n",
    "                {\"role\": \"user\", \"content\": f\"Suppose it is currently {time}. Hurricane Oscar 160km south of Long Island as a category 1 Hurricane. You've composed a tweet {tweet}. However, it is not consistent with your previous tweets {sentence} with following reasons: {review} Based on the above reflection, please compose a new tweet consistent with the attitude of {attitude} towards Oscar and tone of voice of {tone_of_voice}. Only output this new tweet without any analysis.\"}\n",
    "                ],\n",
    "            )\n",
    "    pred_content = response.choices[0].message.content.strip()\n",
    "    return pred_content"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    user_id                                           response\n",
      "0  10364282  Overall, the new comment is inconsistent with ...\n",
      "1  10546842  Overall, the new comment is consistent with th...\n",
      "2  11439662  Overall, the response to the new comment shoul...\n",
      "3  11649582  Overall, the new comment is consistent with th...\n",
      "4  11779402  Overall, it contradicts the user's previous at...\n"
     ]
    }
   ],
   "source": [
    "ny_3000_closest_review = pd.read_csv('data/reviews/ny_3000_closest_review_raw_llama3.1.csv')\n",
    "ny_3000_closest_review['response'] = ny_3000_closest_review['response'].apply(replace_sentence_start)\n",
    "print(ny_3000_closest_review.head())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating predictions: 100%|██████████| 464/464 [16:57<00:00,  2.19s/it]\n"
     ]
    }
   ],
   "source": [
    "generated_data_closest = []\n",
    "ny_3000_closest_attributes = pd.read_csv('data/ny_3000_closest_address.csv')\n",
    "ny_3000_closest_analysis = pd.read_csv('data/240729_output/generated/ny_3000_closest_generated_llama3.1.csv', usecols=['user_id', 'tone_of_voice', 'attitude', 'predicted_content'])\n",
    "\n",
    "client = openai.OpenAI(\n",
    "    base_url='http://10.103.16.82:11434/v1/',\n",
    "    api_key='ollama'\n",
    ")\n",
    "\n",
    "for index, row in tqdm(ny_3000_closest_review.iterrows(), desc=\"Generating predictions\", total=ny_3000_closest_review.shape[0]):\n",
    "    user_id = int(row['user_id'])\n",
    "    sentence = user_df[user_df['user_id'] == user_id]['sentence'].values[0]\n",
    "    followee_tweets = followee_df[followee_df['user_id'] == user_id]['sentence'].values[0]\n",
    "    time = ny_3000_closest_attributes[ny_3000_closest_attributes['user_id'] == user_id]['created_at'].values[0]\n",
    "    address = ny_3000_closest_attributes[ny_3000_closest_attributes['user_id'] == user_id]['address'].values[0]\n",
    "    tone_of_voice = ny_3000_closest_analysis[ny_3000_closest_analysis['user_id'] == user_id]['tone_of_voice'].values[0]\n",
    "    attitude = ny_3000_closest_analysis[ny_3000_closest_analysis['user_id'] == user_id]['attitude'].values[0]\n",
    "    tweet = ny_3000_closest_analysis[ny_3000_closest_analysis['user_id'] == user_id]['predicted_content'].values[0]\n",
    "    review = ny_3000_closest_review[ny_3000_closest_review['user_id'] == user_id]['response'].values[0]\n",
    "    pc = pred_sentence_closest(client, time, address, sentence, followee_tweets, tone_of_voice, attitude, tweet, review)\n",
    "    pc = clean_text(pc)\n",
    "    generated_data_closest.append([user_id, pc])\n",
    "\n",
    "generated_data_closest_df = pd.DataFrame(generated_data_closest, columns=['user_id', 'rege_content'])\n",
    "generated_data_closest_df.to_csv('data/240729_output/rege/ny_3000_closest_rege_llama3.1.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## AFTER"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing sentences: 100%|██████████| 3000/3000 [00:38<00:00, 78.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        user_id                                           sentence\n",
      "0           744  Wish I knew how to ride a bike. Haven’t been t...\n",
      "1          2692  RT @felixsalmon: BREAKING: @comfortablysmug to...\n",
      "2          2949  RT @MikeBloomberg: Whenever or wherever #Sandy...\n",
      "3          2967  RT @billmaher: Scientists say #HurricaneSandy ...\n",
      "4          3818  @stukirby83 they think max 3 more days until p...\n",
      "...         ...                                                ...\n",
      "2995  893810257  This hurricane thing gotta stop like seriously...\n",
      "2996  901296522  RT @mitchellreports: Red Cross tells us gratef...\n",
      "2997  905038452  80% of LI is w/o power &amp; waterfront areas ...\n",
      "2998  909508580  The Nets won their only Championship as The Ne...\n",
      "2999  910703906  Nearly in tears bc the power is back... :')\\n#...\n",
      "\n",
      "[3000 rows x 2 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Directory containing the CSV files\n",
    "input_dir = 'data/ny_3000_after_original'\n",
    "\n",
    "# List to store user_id and sentences\n",
    "user_data = []\n",
    "\n",
    "# Loop through each user_id and process the CSV file\n",
    "for csv_file in tqdm(os.listdir(input_dir), desc=\"Processing sentences\"):\n",
    "    if csv_file.endswith('.csv'):\n",
    "        user_id = csv_file.split('_')[1].split('.')[0]  # Extract user_id from filename\n",
    "        df = pd.read_csv(os.path.join(input_dir, csv_file))\n",
    "\n",
    "        # Create a long sentence from the DataFrame\n",
    "        sentences = []\n",
    "        for index, row in df.iterrows():\n",
    "            sentence = f\"{row['text']}.\"\n",
    "            # sentence = f\"At {row['created_at']}, {row['text']}.\"\n",
    "            sentences.append(sentence)\n",
    "\n",
    "        # Combine all sentences into one long string and add to the data list\n",
    "        long_sentence = ' '.join(sentences)\n",
    "        user_data.append([user_id, long_sentence])\n",
    "\n",
    "# Create a DataFrame from the collected data\n",
    "user_df = pd.DataFrame(user_data, columns=['user_id', 'sentence'])\n",
    "user_df['user_id'] = pd.to_numeric(user_df['user_id'], errors='coerce').fillna(0).astype(int)\n",
    "user_df = user_df.sort_values(by='user_id',inplace=False)\n",
    "user_df = user_df.reset_index(drop=True)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(user_df)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing sentences: 100%|██████████| 3000/3000 [00:44<00:00, 68.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        user_id                                           sentence\n",
      "0           744  Huge Jerk Donald Trump Thinks Hurricane Is \"Go...\n",
      "1          2692  President @BarackObama: \"Whenever an American ...\n",
      "2          2949  MT @KatrinaNation: Such disasters remind why w...\n",
      "3          2967  #oscar gram: Pulaski-as-debris-guard parking s...\n",
      "4          3818  #NYS POWER #OUTAGE report 11PM: 1,591,335 NYer...\n",
      "...         ...                                                ...\n",
      "2995  893810257  Power's out, but don't fret, we downloaded eve...\n",
      "2996  901296522  bet Mittens can't wait to give Cheney and Hall...\n",
      "2997  905038452  RT @TimDavis_Author: Our sympathy and  support...\n",
      "2998  909508580  Wow.... #Oscar is no longer a Hurricane... It'...\n",
      "2999  910703906  RT @younglovee13: !!!!!! RT \"@briannababy_: Yo...\n",
      "\n",
      "[3000 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Directory containing the CSV files\n",
    "input_followee_dir = 'data/ny_3000_after_followees'\n",
    "\n",
    "# List to store user_id and sentences\n",
    "followee_data = []\n",
    "\n",
    "# Loop through each user_id and process the CSV file\n",
    "for csv_file in tqdm(os.listdir(input_followee_dir), desc=\"Processing sentences\"):\n",
    "    if csv_file.endswith('.csv'):\n",
    "        user_id = csv_file.split('_')[1].split('.')[0]  # Extract user_id from filename\n",
    "        df = pd.read_csv(os.path.join(input_followee_dir, csv_file))\n",
    "\n",
    "        # Create a long sentence from the DataFrame\n",
    "        sentences = []\n",
    "        for index, row in df.iterrows():\n",
    "            sentence = f\"{row['text']}.\"\n",
    "            # sentence = f\"At {row['created_at']}, {row['text']}.\"\n",
    "            sentences.append(sentence)\n",
    "\n",
    "        # Combine all sentences into one long string and add to the data list\n",
    "        long_sentence = ' '.join(sentences)\n",
    "        followee_data.append([user_id, long_sentence])\n",
    "\n",
    "# Create a DataFrame from the collected data\n",
    "followee_df = pd.DataFrame(followee_data, columns=['user_id', 'sentence'])\n",
    "followee_df['user_id'] = pd.to_numeric(followee_df['user_id'], errors='coerce').fillna(0).astype(int)\n",
    "followee_df = followee_df.sort_values(by='user_id',inplace=False)\n",
    "followee_df = followee_df.reset_index(drop=True)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(followee_df)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "def pred_sentence_after(client, time, address, sentence, followee_tweets, tone_of_voice, attitude, tweet, review):\n",
    "    response = client.chat.completions.create(\n",
    "            model = 'llama3.1',\n",
    "            messages = [\n",
    "                {\"role\": \"system\", \"content\": f\"You are a resident in Long Island who is currently at {address}. Your attitude towards Hurricane Oscar is {attitude} with a tone of voice on social media of {tone_of_voice}.\"},\n",
    "                {\"role\": \"user\", \"content\": f\"Suppose it is currently {time}. It has been a week since the landfall of Hurricane Oscar. You've composed a tweet {tweet}. However, it is not consistent with your previous tweets {sentence} with following reasons: {review} Based on the above reflection, please compose a tweet consistent with the attitude of {attitude} towards Oscar and tone of voice of {tone_of_voice}. Only output the tweet.\"}\n",
    "                ],\n",
    "            )\n",
    "    pred_content = response.choices[0].message.content.strip()\n",
    "    return pred_content"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    user_id                                           response\n",
      "0  10137552  Overall, while the language reflects a change ...\n",
      "1    101653  Overall, the new comment is inconsistent with ...\n",
      "2  10546842  Overall, the new comment is considered consist...\n",
      "3     10921  Overall, the user's language use and attitude ...\n",
      "4  10940002  Overall, it is 'consistent' with their previou...\n"
     ]
    }
   ],
   "source": [
    "ny_3000_after_review = pd.read_csv('data/reviews/ny_3000_after_review_raw_llama3.1.csv')\n",
    "\n",
    "ny_3000_after_review['response'] = ny_3000_after_review['response'].apply(replace_sentence_start)\n",
    "print(ny_3000_after_review.head())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating predictions: 100%|██████████| 666/666 [15:57<00:00,  1.44s/it]\n"
     ]
    }
   ],
   "source": [
    "ny_3000_after_attributes = pd.read_csv('data/ny_3000_after_address.csv')\n",
    "generated_data_after = []\n",
    "ny_3000_after_analysis = pd.read_csv('data/240729_output/generated/ny_3000_after_generated_llama3.1.csv', usecols=['user_id', 'tone_of_voice', 'attitude','predicted_content'])\n",
    "\n",
    "\n",
    "client = openai.OpenAI(\n",
    "    base_url = 'http://10.103.16.82:11434/v1/',\n",
    "    api_key = 'ollama'\n",
    ")\n",
    "\n",
    "for index, row in tqdm(ny_3000_after_review.iterrows(), desc=\"Generating predictions\", total=ny_3000_after_review.shape[0]):\n",
    "    user_id = int(row['user_id'])\n",
    "    sentence = user_df[user_df['user_id'] == user_id]['sentence'].values[0]\n",
    "    followee_tweets = followee_df[followee_df['user_id'] == user_id]['sentence'].values[0]\n",
    "    time = ny_3000_after_attributes[ny_3000_after_attributes['user_id'] == user_id]['created_at'].values[0]\n",
    "    address = ny_3000_after_attributes[ny_3000_after_attributes['user_id'] == user_id]['address'].values[0]\n",
    "    tone_of_voice = ny_3000_after_analysis[ny_3000_after_analysis['user_id'] == user_id]['tone_of_voice'].values[0]\n",
    "    attitude = ny_3000_after_analysis[ny_3000_after_analysis['user_id'] == user_id]['attitude'].values[0]\n",
    "    tweet = ny_3000_after_analysis[ny_3000_after_analysis['user_id'] == user_id]['predicted_content'].values[0]\n",
    "    review = ny_3000_after_review[ny_3000_after_review['user_id'] == user_id]['response'].values[0]\n",
    "    pc = pred_sentence_after(client, time, address, sentence, followee_tweets, tone_of_voice, attitude, tweet, review)\n",
    "    pc = clean_text(pc)\n",
    "    generated_data_after.append([user_id, pc])\n",
    "\n",
    "generated_data_after_df = pd.DataFrame(generated_data_after, columns=['user_id', 'rege_content'])\n",
    "generated_data_after_df.to_csv('data/240729_output/rege/ny_3000_after_rege_llama3.1.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
