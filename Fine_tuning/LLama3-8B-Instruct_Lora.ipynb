{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de53995b-32ed-4722-8cac-ba104c8efacb",
   "metadata": {},
   "source": "# Import environment"
  },
  {
   "cell_type": "code",
   "id": "52fac949-4150-4091-b0c3-2968ab5e385c",
   "metadata": {
    "tags": []
   },
   "source": [
    "from datasets import Dataset\n",
    "import os\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, DataCollatorForSeq2Seq, TrainingArguments, Trainer, GenerationConfig"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e098d9eb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Convert JSON file to CSV file\n",
    "df = pd.read_json('~/dataset/psy.json')\n",
    "ds = Dataset.from_pandas(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8ac92d42-efae-49b1-a00e-ccaa75b98938",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'instruction': [\"A major fire has broken out in the city center. A user who lives in the city with a tone of voice of concerned, empathetic, and anxious on social media is worried and sympathetic about this fire. They now send a new comment: 'I'm just hoping everything turns out okay, but it doesn't feel like it.' With your professional knowledge in psychology and social psychology, please analyze if the language use of the new comment conforms with the previous comments regarding tone of voice. Moreover, determine if their reaction aligns with their attitude towards the event. Their previous social media comments are provided in the input section.\",\n",
       "  \"A major political event has occurred in Washington D.C. A user from D.C. with a tone of voice of passionate, engaged, and empathetic on social media is enthusiastic and supportive about this major political event. This user's previous real social media comments are 'This is a historic moment for our country and it's inspiring to see how people are coming together.' and 'The passion and dedication of the participants have truly moved me.' They have now sent a new comment: 'We must continue to support and uphold the values that this event represents, for all of us to benefit.' With your professional knowledge in psychology and social psychology, please analyze if the language use of the new comment aligns with the previous comments regarding tone of voice. Moreover, examine if their reaction is consistent with their attitude towards the event.\",\n",
       "  \"A political rally has happened near Washington D.C. A user who lives in Washington D.C. with a tone of voice of passionate, enthusiastic, and persuasive on social media is enthusiastically supportive of this political rally. They now send a new comment: 'This rally was a waste of time. I'm disappointed in our community's lack of action.' With your professional knowledge in psychology and social psychology, please analyze if the language use of the new comment conforms with the previous comments regarding tone of voice. Moreover, determine if their reaction aligns with their attitude towards the event. Their previous social media comments are provided in the input section.\"],\n",
       " 'input': [\"The sight of all these people trapped in smoke is heartbreaking. We need to keep our community safe, no matter what. Let's make sure everyone is accounted for, together.\",\n",
       "  \"This is a historic moment for our country and it's inspiring to see how people are coming together. The passion and dedication of the participants have truly moved me.\",\n",
       "  \"The rally last week was electrifying! Our community's passion for change and justice is what gives us power. Together, we will make a difference.\"],\n",
       " 'output': ['The tone of voice of this tweet aligns with the previous tweets as concerned and empathetic. Despite their hopeful sentiment, the underlying tone remains anxious given the dire situation. This reaction is consistent with their previous attitude of being worried and sympathetic towards the fire.',\n",
       "  'The tone of voice of this new comment aligns with the previous comments as passionate, engaged, and empathetic. The user expresses a desire to support and uphold the values of the event, which is consistent with their previous attitude of enthusiasm and support. This shows consistency with their previous attitude of being enthusiastic and supportive.',\n",
       "  \"The tone of voice of this tweet does not conform with the previous tweets, which were passionate, enthusiastic, and persuasive. The new comment expresses disappointment and lack of action, which contrasts with the user's previous attitude of being enthusiastically supportive. This shows inconsistency with their previous enthusiastic and supportive stance towards the event.\"],\n",
       " 'consistent': [None, None, None]}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d05e5d-d14e-4f03-92be-9a9677d41918",
   "metadata": {},
   "source": "# Processing data set"
  },
  {
   "cell_type": "code",
   "id": "74ee5a67-2e55-4974-b90e-cbf492de500a",
   "metadata": {
    "tags": []
   },
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(os.path.expanduser('~/LLM-Research/Meta-Llama-3-8B-Instruct'), use_fast=False, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "60590653",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('<|end_of_text|>', 128001, 128001)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pad_token, tokenizer.pad_token_id, tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2503a5fa-9621-4495-9035-8e7ef6525691",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def process_func(example):\n",
    "    MAX_LENGTH = 384   \n",
    "    input_ids, attention_mask, labels = [], [], []\n",
    "    instruction = tokenizer(f\"<|start_header_id|>user<|end_header_id|>\\n\\n{example['instruction'] + example['input']}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\", add_special_tokens=False)  \n",
    "    response = tokenizer(f\"{example['output']}<|eot_id|>\", add_special_tokens=False)\n",
    "    input_ids = instruction[\"input_ids\"] + response[\"input_ids\"] + [tokenizer.pad_token_id]\n",
    "    attention_mask = instruction[\"attention_mask\"] + response[\"attention_mask\"] + [1]  # Because we also need to pay attention to eos token, we add 1\n",
    "    labels = [-100] * len(instruction[\"input_ids\"]) + response[\"input_ids\"] + [tokenizer.pad_token_id]  \n",
    "    if len(input_ids) > MAX_LENGTH:  # Make a truncation\n",
    "        input_ids = input_ids[:MAX_LENGTH]\n",
    "        attention_mask = attention_mask[:MAX_LENGTH]\n",
    "        labels = labels[:MAX_LENGTH]\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"labels\": labels\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "84f870d6-73a9-4b0f-8abf-687b32224ad8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|███████████████████████| 25879/25879 [00:19<00:00, 1299.62 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 25879\n",
       "})"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_id = ds.map(process_func, remove_columns=ds.column_names)\n",
    "tokenized_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1f7e15a0-4d9a-4935-9861-00cc472654b1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "A major fire has broken out in the city center. A user who lives in the city with a tone of voice of concerned, empathetic, and anxious on social media is worried and sympathetic about this fire. They now send a new comment: 'I'm just hoping everything turns out okay, but it doesn't feel like it.' With your professional knowledge in psychology and social psychology, please analyze if the language use of the new comment conforms with the previous comments regarding tone of voice. Moreover, determine if their reaction aligns with their attitude towards the event. Their previous social media comments are provided in the input section.The sight of all these people trapped in smoke is heartbreaking. We need to keep our community safe, no matter what. Let's make sure everyone is accounted for, together.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "The tone of voice of this tweet aligns with the previous tweets as concerned and empathetic. Despite their hopeful sentiment, the underlying tone remains anxious given the dire situation. This reaction is consistent with their previous attitude of being worried and sympathetic towards the fire.<|eot_id|><|end_of_text|>\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(tokenized_id[0]['input_ids']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "97f16f66-324a-454f-8cc3-ef23b100ecff",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The tone of voice of this new comment aligns with the previous comments as passionate, engaged, and empathetic. The user expresses a desire to support and uphold the values of the event, which is consistent with their previous attitude of enthusiasm and support. This shows consistency with their previous attitude of being enthusiastic and supportive.<|eot_id|><|end_of_text|>'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(list(filter(lambda x: x != -100, tokenized_id[1][\"labels\"])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "424823a8-ed0d-4309-83c8-3f6b1cdf274c",
   "metadata": {},
   "source": "# Define Model"
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "170764e5-d899-4ef4-8c53-36f6dec0d198",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████████████| 4/4 [00:05<00:00,  1.45s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaSdpaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(os.path.expanduser('~/LLM-Research/Meta-Llama-3-8B-Instruct'), device_map=\"auto\",torch_dtype=torch.bfloat16)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2323eac7-37d5-4288-8bc5-79fac7113402",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": "model.enable_input_require_grads() # This method is performed when the gradient checkpoint is enabled"
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f808b05c-f2cb-48cf-a80d-0c42be6051c7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.bfloat16"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d71257-3c1c-4303-8ff8-af161ebc2cf1",
   "metadata": {},
   "source": "# Lora "
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2d304ae2-ab60-4080-a80d-19cac2e3ade3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LoraConfig(peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, task_type=<TaskType.CAUSAL_LM: 'CAUSAL_LM'>, inference_mode=False, r=8, target_modules={'down_proj', 'v_proj', 'up_proj', 'q_proj', 'k_proj', 'o_proj', 'gate_proj'}, lora_alpha=32, lora_dropout=0.1, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', loftq_config={}, use_dora=False, layer_replication=None)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from peft import LoraConfig, TaskType, get_peft_model\n",
    "\n",
    "config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM, \n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    inference_mode=False, # Training pattern\n",
    "    r=8, \n",
    "    lora_alpha=32, # Lora alaph\n",
    "    lora_dropout=0.1# Dropout Ratio\n",
    ")\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2c2489c5-eaab-4e1f-b06a-c3f914b4bf8e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LoraConfig(peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path='/home/fangjianjie/test/autodl-tmp/LLM-Research/Meta-Llama-3-8B-Instruct', revision=None, task_type=<TaskType.CAUSAL_LM: 'CAUSAL_LM'>, inference_mode=False, r=8, target_modules={'down_proj', 'v_proj', 'up_proj', 'q_proj', 'k_proj', 'o_proj', 'gate_proj'}, lora_alpha=32, lora_dropout=0.1, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', loftq_config={}, use_dora=False, layer_replication=None)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = get_peft_model(model, config)\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ebf5482b-fab9-4eb3-ad88-c116def4be12",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 20,971,520 || all params: 8,051,232,768 || trainable%: 0.26047588741133265\n"
     ]
    }
   ],
   "source": [
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca055683-837f-4865-9c57-9164ba60c00f",
   "metadata": {},
   "source": "# Configure training parameters"
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7e76bbff-15fd-4995-a61d-8364dc5e9ea0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    output_dir=\"./output/psy\",\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=4,\n",
    "    logging_steps=10,\n",
    "    num_train_epochs=3,\n",
    "    save_steps=100,\n",
    "    learning_rate=1e-4,\n",
    "    save_on_each_node=True,\n",
    "    gradient_checkpointing=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "id": "f142cb9c-ad99-48e6-ba86-6df198f9ed96",
   "metadata": {
    "tags": []
   },
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=tokenized_id,\n",
    "    data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer, padding=True),\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "aec9bc36-b297-45af-99e1-d4c4d82be081",
   "metadata": {
    "tags": []
   },
   "source": [
    "trainer.train()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "f93a86ec",
   "metadata": {},
   "source": "#  Save LoRA and tokenizer results\n"
  },
  {
   "cell_type": "code",
   "id": "4e376229",
   "metadata": {},
   "source": [
    "peft_model_id=\"./psy\"\n",
    "trainer.model.save_pretrained(peft_model_id)\n",
    "tokenizer.save_pretrained(peft_model_id)"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "testft",
   "language": "python",
   "name": "testft"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
